{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79cb19b",
   "metadata": {},
   "source": [
    "# This is a tutorial for machine translation with T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820aaa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9b0481f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-base\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11fcbc",
   "metadata": {},
   "source": [
    "## We will use pretrained t5-small model to finetune a English to French model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ff8b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use bleu score as the evaluation metric\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdafa3a8",
   "metadata": {},
   "source": [
    "## First we need to read the files and convert it into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc509da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('chn_parallel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6fc8573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>sim</th>\n",
       "      <th>tra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On Monday, scientists from the Stanford Univer...</td>\n",
       "      <td>周一，斯坦福大学医学院的科学家宣布，他们发明了一种可以将细胞按类型分类的新型诊断工具：一种可...</td>\n",
       "      <td>史丹佛大學醫學院的科學家於週一宣布發明一項新型診斷工具，可依類型將細胞分類：這是一種細小的可...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lead researchers say this may bring early dete...</td>\n",
       "      <td>主要研究人员表示，这可以让低收入国家/地区的患者尽早发现癌症、肺结核、艾滋病和疟疾。在这些国...</td>\n",
       "      <td>主要研究人員表示，這或許可以讓低收入國家的癌症、肺結核、愛滋病毒及瘧疾病患早期發現病症。在這...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The JAS 39C Gripen crashed onto a runway at ar...</td>\n",
       "      <td>当地时间上午 9:30 左右 (UTC 0230)，JAS 39C 鹰狮战斗机撞上跑道并发生...</td>\n",
       "      <td>JAS 39C 獅鷲戰鬥機在當地時間上午 9 點 30 分（世界協調時間 02:30）墜落在...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The pilot was identified as Squadron Leader Di...</td>\n",
       "      <td>涉事飞行员是空军中队长迪罗里·帕塔维 (Dilokrit Pattavee)。</td>\n",
       "      <td>駕駛員的身分確認是空軍少校帕塔維 (Dilokrit Pattavee)。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Local media reports an airport fire vehicle ro...</td>\n",
       "      <td>当地媒体报道，一辆机场消防车在响应火警时翻了车。</td>\n",
       "      <td>當地媒體報導一輛機場消防車在出勤時翻覆。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>The tourist season for the hill stations gener...</td>\n",
       "      <td>山中避暑之地的旅游旺季通常是在印度的夏季。</td>\n",
       "      <td>避暑勝地的旅遊季節，通常在印度的夏季期間達到頂峰。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>However, they have a different kind of beauty ...</td>\n",
       "      <td>然而，冬天却另有一番不同的美景和魅力，许多山中避暑之地的雪量恰到好处。这些地方会提供活动项目...</td>\n",
       "      <td>然而，它們在冬天有不同的美與魅力。許多山間小鎮都會下不少的雪，且那裡也會提供滑雪和滑雪板等活動。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Only a few airlines still offer bereavement fa...</td>\n",
       "      <td>只有少数几家航空公司扔提供丧亲票价，这比为赶赴葬礼的临时出行稍微便宜一些。</td>\n",
       "      <td>只有少數航空公司仍然提供喪親票價，為臨時的葬禮旅行提供些微折扣。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Airlines that offer these include Air Canada, ...</td>\n",
       "      <td>提供此类票价的航空公司包括加拿大航空公司、达美航空公司、德国汉莎航空公司（从美国或加拿大起飞...</td>\n",
       "      <td>提供這些服務的航空公司包括加拿大航空、達美航空、漢莎航空（從美國或加拿大出發的航班）和西捷航空。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>In all cases, you must book by phone directly ...</td>\n",
       "      <td>无论情况如何，你都必须直接打电话向航空公司订票。</td>\n",
       "      <td>不論任何情況，都必須打電話直接跟航空公司訂位。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>997 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   eng  \\\n",
       "0    On Monday, scientists from the Stanford Univer...   \n",
       "1    Lead researchers say this may bring early dete...   \n",
       "2    The JAS 39C Gripen crashed onto a runway at ar...   \n",
       "3    The pilot was identified as Squadron Leader Di...   \n",
       "4    Local media reports an airport fire vehicle ro...   \n",
       "..                                                 ...   \n",
       "992  The tourist season for the hill stations gener...   \n",
       "993  However, they have a different kind of beauty ...   \n",
       "994  Only a few airlines still offer bereavement fa...   \n",
       "995  Airlines that offer these include Air Canada, ...   \n",
       "996  In all cases, you must book by phone directly ...   \n",
       "\n",
       "                                                   sim  \\\n",
       "0    周一，斯坦福大学医学院的科学家宣布，他们发明了一种可以将细胞按类型分类的新型诊断工具：一种可...   \n",
       "1    主要研究人员表示，这可以让低收入国家/地区的患者尽早发现癌症、肺结核、艾滋病和疟疾。在这些国...   \n",
       "2    当地时间上午 9:30 左右 (UTC 0230)，JAS 39C 鹰狮战斗机撞上跑道并发生...   \n",
       "3              涉事飞行员是空军中队长迪罗里·帕塔维 (Dilokrit Pattavee)。   \n",
       "4                             当地媒体报道，一辆机场消防车在响应火警时翻了车。   \n",
       "..                                                 ...   \n",
       "992                              山中避暑之地的旅游旺季通常是在印度的夏季。   \n",
       "993  然而，冬天却另有一番不同的美景和魅力，许多山中避暑之地的雪量恰到好处。这些地方会提供活动项目...   \n",
       "994              只有少数几家航空公司扔提供丧亲票价，这比为赶赴葬礼的临时出行稍微便宜一些。   \n",
       "995  提供此类票价的航空公司包括加拿大航空公司、达美航空公司、德国汉莎航空公司（从美国或加拿大起飞...   \n",
       "996                           无论情况如何，你都必须直接打电话向航空公司订票。   \n",
       "\n",
       "                                                   tra  \n",
       "0    史丹佛大學醫學院的科學家於週一宣布發明一項新型診斷工具，可依類型將細胞分類：這是一種細小的可...  \n",
       "1    主要研究人員表示，這或許可以讓低收入國家的癌症、肺結核、愛滋病毒及瘧疾病患早期發現病症。在這...  \n",
       "2    JAS 39C 獅鷲戰鬥機在當地時間上午 9 點 30 分（世界協調時間 02:30）墜落在...  \n",
       "3                駕駛員的身分確認是空軍少校帕塔維 (Dilokrit Pattavee)。  \n",
       "4                                 當地媒體報導一輛機場消防車在出勤時翻覆。  \n",
       "..                                                 ...  \n",
       "992                          避暑勝地的旅遊季節，通常在印度的夏季期間達到頂峰。  \n",
       "993   然而，它們在冬天有不同的美與魅力。許多山間小鎮都會下不少的雪，且那裡也會提供滑雪和滑雪板等活動。  \n",
       "994                   只有少數航空公司仍然提供喪親票價，為臨時的葬禮旅行提供些微折扣。  \n",
       "995   提供這些服務的航空公司包括加拿大航空、達美航空、漢莎航空（從美國或加拿大出發的航班）和西捷航空。  \n",
       "996                            不論任何情況，都必須打電話直接跟航空公司訂位。  \n",
       "\n",
       "[997 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "396f9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a customized dataset class\n",
    "class CustomDataset():\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sim = list(dataframe['sim'])\n",
    "        self.tra = list(dataframe['tra'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sim)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #we need to get the input ids of input(English) and output(French)\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            self.sim[index],\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        outputs = self.tokenizer.encode_plus(\n",
    "            self.tra[index],\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        labels=outputs['input_ids']   \n",
    "        return torch.tensor(input_ids, dtype=torch.long),torch.tensor(attention_mask, dtype=torch.long),torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5eb5ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split and create dataloders\n",
    "\n",
    "#train, test = train_test_split(df, test_size=0.5,random_state=12345)\n",
    "\n",
    "train_set = CustomDataset(df, tokenizer)\n",
    "trainloader = DataLoader(train_set, batch_size=2,shuffle=True)\n",
    "#we only randomly pick 2000 samples as test_set\n",
    "#test_set = CustomDataset(test[:2000], tokenizer)\n",
    "#testloader = DataLoader(test_set, batch_size=2,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "278e0552",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#ground truth of test_set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m true_list\u001b[38;5;241m=\u001b[39m\u001b[43mtest\u001b[49m[:\u001b[38;5;241m2000\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfra\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "#ground truth of test_set\n",
    "true_list=test[:2000]['fra'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41868202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_loader, model,optimizer):\n",
    "    \"\"\"one epoch training\"\"\"\n",
    "\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    for idx, (input_ids,attention_mask,labels) in enumerate(train_loader):\n",
    "\n",
    "\n",
    "        # replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "        labels[labels==0]=-100\n",
    "        if torch.cuda.is_available():\n",
    "            input_ids = input_ids.cuda()\n",
    "            attention_mask = attention_mask.cuda()\n",
    "            labels= labels.cuda()\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        #calculate the loss \n",
    "        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "        \n",
    "        #accumulate the loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "\n",
    "\n",
    "        # SGD\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    return epoch_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78cd0e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating(eval_loader, model,optimizer):\n",
    "    epoch_loss = 0\n",
    "    pred_list=[]\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for idx, (input_ids,attention_mask,labels) in enumerate(eval_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                input_ids = input_ids.cuda()\n",
    "                attention_mask = attention_mask.cuda()\n",
    "            \n",
    "            \n",
    "            #get the output sequence\n",
    "            output_sequences = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            outputs=tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
    "            for samples in outputs:\n",
    "                pred_list.append(samples)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #calculate bleu score\n",
    "    bleu_score=bleu.compute(predictions=pred_list, references=true_list)    \n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "432bebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "#Set the optimizer and learning rate is recommended to be 1e-4 by huggingface\n",
    "optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=1e-4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e863c57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 8.00 GiB total capacity; 6.66 GiB already allocated; 0 bytes free; 6.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#training\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m tr_loss\u001b[38;5;241m=\u001b[39m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_loss:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mround\u001b[39m(tr_loss, \u001b[38;5;241m5\u001b[39m)))\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(train_loader, model, optimizer)\u001b[0m\n\u001b[0;32m     17\u001b[0m     labels\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#calculate the loss \u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#accumulate the loss\u001b[39;00m\n\u001b[0;32m     26\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1676\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtie_word_embeddings:\n\u001b[0;32m   1672\u001b[0m     \u001b[38;5;66;03m# Rescale output before projecting on vocab\u001b[39;00m\n\u001b[0;32m   1673\u001b[0m     \u001b[38;5;66;03m# See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\u001b[39;00m\n\u001b[0;32m   1674\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m sequence_output \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dim\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m-> 1676\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1678\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 978.00 MiB (GPU 0; 8.00 GiB total capacity; 6.66 GiB already allocated; 0 bytes free; 6.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "for i in range(5):\n",
    "\n",
    "\n",
    "    print('epochs:'+ str(i+1))\n",
    "    \n",
    "    #training\n",
    "    tr_loss=training(trainloader, model,optimizer)\n",
    "    print('training_loss:'+str(round(tr_loss, 5)))\n",
    "\n",
    "    #evaluating\n",
    "    #bleu_score=evaluating(testloader, model,optimizer)\n",
    "    #print('bleu_score:'+str(round(bleu_score['bleu'], 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61139aa",
   "metadata": {},
   "source": [
    "## Due to the large amount of training data. The training is done on a cluster.\n",
    "we can run evaluating function again to check the bleu score after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9387220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_score=evaluating(testloader, model,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1500aa65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.4875162591403384,\n",
       " 'precisions': [0.7436706689536878,\n",
       "  0.5589662027833002,\n",
       "  0.4481323877068558,\n",
       "  0.3667598416026089],\n",
       " 'brevity_penalty': 0.9535654925674059,\n",
       " 'length_ratio': 0.9546109510086456,\n",
       " 'translation_length': 14575,\n",
       " 'reference_length': 15268}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543e4f18",
   "metadata": {},
   "source": [
    "## a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e34008fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HuggingFace est une société.', 'Bienvenue à NYC.']\n"
     ]
    }
   ],
   "source": [
    "#Check the model outputs after training\n",
    "sentences = [\"HuggingFace is a company.\", \"Welcome to NYC.\"]\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
    "output_sequences = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    ")\n",
    "print(tokenizer.batch_decode(output_sequences, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "508013c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.5969491792019646,\n",
       " 'precisions': [0.8888888888888888,\n",
       "  0.7142857142857143,\n",
       "  0.6,\n",
       "  0.3333333333333333],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.0,\n",
       " 'translation_length': 9,\n",
       " 'reference_length': 9}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.compute(predictions=['HuggingFace est une société.', 'Bienvenue à NYC.'], references=[\"HuggingFace est une entreprise.\", \"Bienvenue à NYC.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6c7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
