{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "solbqGQKomLY"
      },
      "source": [
        "## Week 7 \"Mixture of Experts\"\n",
        "\n",
        "Welcome to the Tutorial 7. We will discuss about the basic of Mixture of Expert (MoE).\n",
        "\n",
        "The objectives of this lab are as follows:\n",
        "1. Technical Understanding about Hard and Soft MoE. MoE is a learning method that can be applied in any field. To do so, we will start with vanilla Multilayer Perceptron model in MNIST.\n",
        "2. How to implement MoE in Transformers.\n",
        "3. Make you understand about the basic technical idea about Switch Transformers.\n",
        "\n",
        "As always, feel free to ask us :)\n",
        "\n",
        "Assistant: \\\\\n",
        "Farid - farid.adilazuarda@mbzuai.ac.ae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K1Zy7ZcomLZ"
      },
      "source": [
        "## Helper and Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-jtC1XYoyJH"
      },
      "outputs": [],
      "source": [
        "!pip install Lightning\n",
        "!pip install datasets\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qXBDuQkomLZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from lightning import LightningModule\n",
        "from lightning import Trainer as LightningTrainer\n",
        "\n",
        "import matplotlib.gridspec as gridspec\n",
        "import pandas as pd\n",
        "\n",
        "import torchvision\n",
        "from torchmetrics import Accuracy\n",
        "from copy import deepcopy\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding, SwitchTransformersConfig\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from typing import Tuple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsjK9t2JomLZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# for every input, check its best expert with the predicted class\n",
        "def plot_figure(x, y, y_hat, weights):\n",
        "    # add pad for better visualization\n",
        "    fig = plt.figure(figsize=(20, 20))\n",
        "\n",
        "    # Create a 5x5 grid for the main layout\n",
        "    outer_grid = gridspec.GridSpec(5, 5, wspace=0.4, hspace=0.4)\n",
        "\n",
        "    # plot the bar chart for the weights and also the predicted class and the label\n",
        "    # additionally, show the image\n",
        "    for i in range(5):\n",
        "        for j in range(5):\n",
        "\n",
        "            idx = i * 5 + j  # Adjusted to properly index through a 5x5 grid\n",
        "\n",
        "            # Create a nested grid for each subplot (2 columns: one for image, one for bar chart)\n",
        "            inner_grid = gridspec.GridSpecFromSubplotSpec(1, 2, subplot_spec=outer_grid[i, j], wspace=0.1, width_ratios=[3, 2])\n",
        "\n",
        "            # First subplot for the image\n",
        "            ax1 = plt.Subplot(fig, inner_grid[0])\n",
        "            ax1.imshow(x[idx].view(28, 28), cmap='gray')  # Uncomment and adjust according to your data\n",
        "            ax1.set_xticks([])\n",
        "            ax1.set_yticks([])\n",
        "            fig.add_subplot(ax1)\n",
        "\n",
        "            # Second subplot for the bar chart\n",
        "            ax2 = plt.Subplot(fig, inner_grid[1])\n",
        "            weights_example =  weights[idx]  #\n",
        "            y_hat_example = y_hat[idx].argmax()  #\n",
        "            y_example = y[idx]  #\n",
        "            expert_example = weights_example.argmax()\n",
        "\n",
        "            ax2.bar(np.arange(5), weights_example, color='blue', tick_label=np.arange(5))\n",
        "            ax2.set_title(f'Predicted class: {y_hat_example}')\n",
        "            ax2.set_xlabel(f'Label: {y_example}, Expert: {expert_example}')\n",
        "            fig.add_subplot(ax2)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def extract_moe_contribution(y, y_hat, weights):\n",
        "    returned_list = []\n",
        "    for i in range(len(y)):\n",
        "        returned_list.append({\n",
        "            \"label\": y[i].item(),\n",
        "            \"predicted\": y_hat[i].argmax().item(),\n",
        "            \"best_experts\": weights[i].argmax().item()\n",
        "        })\n",
        "    return returned_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J9LXGeRomLZ"
      },
      "outputs": [],
      "source": [
        "## Create Our Classification Flow\n",
        "\n",
        "# train model\n",
        "class LitClassification(LightningModule):\n",
        "\n",
        "    def __init__(self, model, do_output_weights: bool = False):\n",
        "        super(LitClassification, self).__init__()\n",
        "        self.model = model\n",
        "        self.do_output_weights = do_output_weights\n",
        "        self.loss = torch.nn.CrossEntropyLoss()\n",
        "        self.train_acc = Accuracy('multiclass', num_classes=10)\n",
        "        self.test_acc = Accuracy('multiclass', num_classes=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self.model(x)\n",
        "        if self.do_output_weights:\n",
        "            y_hat, _ = y_hat\n",
        "\n",
        "        loss = self.loss(y_hat, y)\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_acc', self.train_acc(y_hat, y), prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self.model(x)\n",
        "        if self.do_output_weights:\n",
        "            y_hat, _ = y_hat\n",
        "        loss = self.loss(y_hat, y)\n",
        "        self.log('test_loss', loss)\n",
        "        self.log('test_acc', self.test_acc(y_hat, y), prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
        "\n",
        "def check_expert_acc(moemodel, test_loader, lit_test_clf, trainer):\n",
        "    lit_test_clf.eval()\n",
        "    print(\"Expert accuracy\")\n",
        "    for i, expert in enumerate(moemodel.model.experts):\n",
        "        with torch.no_grad():\n",
        "            print(f\"Expert {i}\")\n",
        "            lit_test_clf.model = expert\n",
        "            trainer.test(lit_test_clf, test_loader)\n",
        "\n",
        "def plot_a_single_batch(x, y, y_hat, weights):\n",
        "    with torch.no_grad():\n",
        "        plot_figure(x, y, y_hat, weights)\n",
        "\n",
        "def check_prediction_contribution(moemodel, test_loader):\n",
        "    contributions = []\n",
        "    for x, y in test_loader:\n",
        "        y_hat, weights = moemodel.model(x, output_proba_gates=True)\n",
        "        contributions.extend(extract_moe_contribution(y, y_hat, weights))\n",
        "    df = pd.DataFrame(contributions)\n",
        "    for i in range(5):\n",
        "        total_expert_contribution = df[df.best_experts == i].shape[0]\n",
        "        print(f\"Expert {i}\")\n",
        "        print(f\"Total contribution: {total_expert_contribution}\")\n",
        "        if total_expert_contribution == 0:\n",
        "            continue\n",
        "        predicted = df[df.best_experts == i].predicted.value_counts()\n",
        "        print(predicted)\n",
        "        # check if the expert is always predicting the same class\n",
        "        print(\"Precision based on the best expert:\")\n",
        "        print(df[df.best_experts == i][df.predicted == df.label].shape[0] / df[df.best_experts == i].shape[0])\n",
        "\n",
        "\n",
        "def probe_result(trainer, moemodel, test_loader, moe_obj):\n",
        "    trainer.test(moemodel, test_loader)\n",
        "    ## Check each expert accuracy\n",
        "    lit_test_clf = LitClassification(moe_obj, do_output_weights=False)\n",
        "    check_expert_acc(moemodel, test_loader, lit_test_clf, trainer)\n",
        "\n",
        "    print(\"Check the contribution of each expert\")\n",
        "    # check forward for a single batch\n",
        "    x, y = next(iter(test_loader))\n",
        "    y_hat, weights = moemodel.model(x, output_proba_gates=True)\n",
        "\n",
        "    # convert them to numpy\n",
        "    y_hat = y_hat.detach().numpy()\n",
        "    weights = weights.detach().numpy()\n",
        "\n",
        "    # plot the figure\n",
        "    plot_a_single_batch(x, y, y_hat, weights)\n",
        "\n",
        "    # loop through the test loader and extract the contribution of each expert\n",
        "    print(\"Extracting the contribution of each expert\")\n",
        "    check_prediction_contribution(moemodel, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YeTlRk9omLa"
      },
      "source": [
        "# Initialize our basic expert model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKpS9bnWomLa"
      },
      "outputs": [],
      "source": [
        "class ExpertModel(torch.nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "      super(ExpertModel, self).__init__()\n",
        "      self.in_features = in_features\n",
        "      self.out_features = out_features\n",
        "      self.layer1 = torch.nn.Linear(in_features, out_features)\n",
        "      self.layer2 = torch.nn.Linear(out_features, out_features)\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "      x = torch.nn.functional.relu(self.layer1(x))\n",
        "      x = self.layer2(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "def generate_n_experts(n: int, in_features: int, out_features: int):\n",
        "    # put your code here to generate experts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPpsrjqMomLa"
      },
      "source": [
        "## Initialize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC3XxyQYomLa"
      },
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Lambda(lambda x: x.view(-1))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHHskiAuomLa"
      },
      "source": [
        "## Define our MoE objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luuFXkl3omLa"
      },
      "source": [
        "Illustration of MoE:\n",
        "\n",
        "![image.png](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhbsjq4iseHi-Azxcj0irBjGkma0yd4geSPPombnJSdd5dyzTguUU2pdFfZu4G38G4F4TiymUOaIkQnXGVAix5x8wF3-9Ov3NJwWaEZNvJY84CWCgU5MbUYI_DjKa_BvalTHu3eyfCJGR89UqwskKngsppDy94Gahz3HAoKLh2vmh-Jzb7ZedRI91OwFw/s960/image1.jpg)\n",
        "\n",
        "From: https://blog.research.google/2022/11/mixture-of-experts-with-expert-choice.html?m=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLjXW3ucomLa"
      },
      "source": [
        "To describe the Mixture of Experts (MoE) model based on the provided Python class definitions, we can break it down into the algorithm's key components and functionalities. The MoE model typically consists of two main parts: a gating mechanism to decide the weighting of each expert's output, and a set of expert models that provide specialized knowledge or processing.\n",
        "\n",
        "You may implement these:\n",
        "\n",
        "### 1. **Gate Layer** (AKA Router)\n",
        "\n",
        "The Gate Layer's primary role is to dynamically allocate input data to different experts based on the input features. This is achieved through a two-layer neural network with a softmax activation on the output layer to ensure that the weights sum up to one.\n",
        "\n",
        "Given an input tensor $x \\in \\mathbb{R}^{batch\\_size \\times in\\_features}$, the gating mechanism works as follows:\n",
        "\n",
        "1. **First Layer Transformation**: Apply a linear transformation followed by a ReLU activation function.\n",
        "    $$\n",
        "    x' = ReLU(W_1 x + b_1)\n",
        "    $$\n",
        "    where $W_1 \\in \\mathbb{R}^{hidden\\_size \\times in\\_features}$ and $b_1 \\in \\mathbb{R}^{hidden\\_size}$ are the weights and biases of the first layer.\n",
        "\n",
        "2. **Second Layer Transformation**: Apply another linear transformation to map to the space of experts.\n",
        "    $$\n",
        "    z = W_2 x' + b_2\n",
        "    $$\n",
        "    where $W_2 \\in \\mathbb{R}^{num\\_experts \\times hidden\\_size}$ and $b_2 \\in \\mathbb{R}^{num\\_experts}$.\n",
        "\n",
        "3. **Softmax Activation**: The softmax function is applied to obtain the gating probabilities, which indicate the weight of each expert's contribution.\n",
        "    $$\n",
        "    \\text{gates} = Softmax(z) = \\frac{e^{z_i}}{\\sum_{j=1}^{num\\_experts} e^{z_j}}\n",
        "    $$\n",
        "    for each expert $i$, ensuring the output is a probability distribution over experts.\n",
        "\n",
        "### 2. **MoE Layer**\n",
        "\n",
        "The MoE Layer orchestrates the overall processing, combining the expert models' outputs based on the gating probabilities. For an input $x$, the process is as follows:\n",
        "\n",
        "1. **Gating Probabilities**: Compute the gating probabilities for each expert as described above.\n",
        "2. **Expert Processing**: Each expert model processes the input independently.\n",
        "    $$\n",
        "    E_i(x) \\quad \\text{for each expert} \\; i\n",
        "    $$\n",
        "3. **Output Combination**: The outputs of the experts are combined based on the gating probabilities. This can be represented as a weighted sum:\n",
        "    $$\n",
        "    O(x) = \\sum_{i=1}^{num\\_experts} \\text{gates}_i \\cdot E_i(x)\n",
        "    $$\n",
        "    where \\(O(x)\\) is the final output for the input $x$, and $\\text{gates}_i$ is the gating probability for expert $i$.\n",
        "\n",
        "This framework allows the MoE model to leverage the strengths of different experts for different parts of the input space, potentially improving the model's overall performance on complex tasks with diverse data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zynwhxqcomLa"
      },
      "outputs": [],
      "source": [
        "class GateLayer(torch.nn.Module):\n",
        "    def __init__(self, in_features, hidden_size, num_experts):\n",
        "        super(GateLayer, self).__init__()\n",
        "        # continue the main attributes for the gate layer\n",
        "\n",
        "class MoELayer(torch.nn.Module):\n",
        "    def __init__(self, experts: list[ExpertModel], gate_layer: GateLayer, is_sequence: bool = True):\n",
        "        super(MoELayer, self).__init__()\n",
        "        # clone expert model to create num_experts copies and reset their parameters\n",
        "        self.experts = torch.nn.ModuleList(experts)\n",
        "        self.num_experts = len(experts)\n",
        "\n",
        "        self.gate_layer = gate_layer\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.is_sequence = is_sequence\n",
        "\n",
        "        self.permutation_dimension = (1,2,0,3) if is_sequence else (1,0,2)\n",
        "\n",
        "\n",
        "    def forward(self, x, output_proba_gates=False):\n",
        "        # implement the forward function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMqJ2o2somLa"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "gate = GateLayer(\n",
        "    in_features=784,\n",
        "    hidden_size=100,\n",
        "    num_experts=5,\n",
        ")\n",
        "\n",
        "experts = generate_n_experts(\n",
        "    n=5,\n",
        "    in_features=784,\n",
        "    out_features=10\n",
        ")\n",
        "\n",
        "moe = MoELayer(\n",
        "    experts=experts,\n",
        "    gate_layer=gate,\n",
        "    is_sequence=False\n",
        ")\n",
        "\n",
        "moemodel = LitClassification(moe)\n",
        "\n",
        "trainer = LightningTrainer(max_epochs=1)\n",
        "\n",
        "trainer.fit(moemodel, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-uTTo4ComLb"
      },
      "outputs": [],
      "source": [
        "trainer.test(moemodel, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO4S0JWeomLb"
      },
      "outputs": [],
      "source": [
        "probe_result(trainer, moemodel, test_loader, moe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAUwohk0omLb"
      },
      "source": [
        "## Hard Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcX3t8cZomLb"
      },
      "source": [
        "Below is a variant of the gating mechanism in a Mixture of Experts (MoE) model, focusing on selecting only the top \\(k\\) experts for each input. This approach is designed to enhance the model's efficiency and effectiveness by concentrating computation on the most relevant experts for a given input. Here is an explanation of how this gating mechanism works, incorporating noise for load balancing and the selection of top \\(k\\) experts.\n",
        "\n",
        "Implement these inside the class:\n",
        "\n",
        "\n",
        "### **Tunable Noise for Load Balancing**\n",
        "\n",
        "The `_get_tunable_noise` method introduces a novel aspect of the `GateLayerTopK`: adding tunable noise to the gating mechanism. This noise is weighted by the input data and a learnable weight parameter (`w_noise`), intended to diversify the inputs to the experts and balance their workload. Use any arbitrary noise. The noise is calculated as follows:\n",
        "\n",
        "$$\n",
        "\\text{weighted_noise} = \\text{ReLU}(XW_{noise})\n",
        "$$\n",
        "\n",
        "where $(X)$ is the input tensor and $(W_{noise})$ is the learned noise weight matrix. The noise added to the gating signal is then a random noise tensor, scaled by this weighted noise.\n",
        "\n",
        "### 3. **Forward Pass**\n",
        "\n",
        "During the forward pass, the input \\(X\\) undergoes several transformations:\n",
        "\n",
        "1. **Initial Processing**: The input is first passed through a ReLU-activated linear layer (`layer1`), then through another linear transformation (`layer2`).\n",
        "\n",
        "2. **Noise Addition**: Tunable noise is added to the gating signals to encourage load balancing among experts.\n",
        "\n",
        "4. **Top-k Expert Selection**:\n",
        "   - The gating mechanism is adjusted to only consider the top $(k)$ experts for each input. This is done by setting the scores of all but the top $(k)$ experts to $(-inf)$ using the following steps:\n",
        "     1. Identify the indices of the bottom $(n-k)$ experts (where $(n)$ is the total number of experts) for each input based on their gating scores.\n",
        "     2. Use the `scatter` function to set the scores of these bottom $(n-k)$ experts to $-inf$, effectively excluding them from the final softmax calculation.\n",
        "   \n",
        "5. **Softmax Activation**: Finally, a softmax is applied to the adjusted gating scores, ensuring that the output is a sparse probability distribution that sums to one, focusing solely on the top $(k)$ experts for each input.\n",
        "\n",
        "### Summary\n",
        "\n",
        "The `GateLayerTopK` introduces a selective and noise-enhanced gating mechanism for MoE models, aiming to improve both the efficiency of computation by focusing on the most relevant experts and the diversity of expert utilization through tunable noise. This approach can lead to better performance and more balanced utilization of experts in complex tasks.\n",
        "\n",
        "### Nice to try\n",
        "\n",
        "Play around with the top-k, what do you find?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpqR5mEoomLb"
      },
      "outputs": [],
      "source": [
        "# We select k experts here\n",
        "# We modify the gate\n",
        "\n",
        "class GateLayerTopK(torch.nn.Module):\n",
        "    def __init__(self, in_features, hidden_size, num_experts, top_k: int):\n",
        "        super(GateLayerTopK, self).__init__()\n",
        "        self.top_k = top_k\n",
        "        self.in_features = in_features\n",
        "        self.num_experts = num_experts\n",
        "        self.layer1 = torch.nn.Linear(in_features, hidden_size)\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "        self.layer2 = torch.nn.Linear(hidden_size, num_experts)\n",
        "        self.w_g = torch.nn.Parameter(torch.zeros(hidden_size, num_experts))\n",
        "        self.w_noise = torch.nn.Parameter(torch.zeros(in_features, num_experts))\n",
        "\n",
        "\n",
        "    def _get_tunable_noise(self, x):\n",
        "        \"\"\"Add noise to the input for load balancing.\"\"\"\n",
        "        weighted_noise = torch.matmul(x, self.w_noise)\n",
        "        return torch.randn_like(weighted_noise) * weighted_noise\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        noise = self._get_tunable_noise(x)\n",
        "        x = torch.nn.functional.relu(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        x = x + noise\n",
        "\n",
        "        # Addition\n",
        "        # Implement selection for the top k experts\n",
        "\n",
        "\n",
        "        ## End of addition\n",
        "\n",
        "        return self.softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C90lBtBxomLb"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "gate2 = GateLayerTopK(\n",
        "    in_features=784,\n",
        "    hidden_size=100,\n",
        "    num_experts=5,\n",
        "    top_k=2,\n",
        ")\n",
        "\n",
        "experts2 = generate_n_experts(\n",
        "    n=5,\n",
        "    in_features=784,\n",
        "    out_features=10\n",
        ")\n",
        "\n",
        "moe2 = MoELayer(\n",
        "    experts=experts2,\n",
        "    gate_layer=gate2,\n",
        "    is_sequence=False\n",
        ")\n",
        "\n",
        "moemodel2 = LitClassification(moe2)\n",
        "\n",
        "trainer = LightningTrainer(max_epochs=1)\n",
        "\n",
        "trainer.fit(moemodel2, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjsfH7QIomLb"
      },
      "outputs": [],
      "source": [
        "trainer.test(moemodel2, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hpCtWDQ5v1lV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjgbM3GLomLb"
      },
      "outputs": [],
      "source": [
        "probe_result(trainer, moemodel2, test_loader, moe2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl4H37R-omLb"
      },
      "source": [
        "## Load Balancing\n",
        "\n",
        "As you see above, some experts have smaller contributions. If you want to make the experts work equally, you can do load balancing.\n",
        "\n",
        "There are several ways to do this. We will pick this algorithm instead (From [This paper](https://arxiv.org/abs/1312.4314))\n",
        "\n",
        "To provide a more detailed explanation of the `GateLayerTopKLoadBalancing` class with equations, let's break down its functionality into mathematical components:\n",
        "\n",
        "\n",
        "#### 5. **Load Balancing (Optional)**\n",
        "\n",
        "If load balancing is enabled, the following steps are taken to adjust the expert weights based on historical load:\n",
        "\n",
        "- Compute the step's total assignment to each expert:\n",
        "\n",
        "$ \\text{step_total_assignment} = \\sum_{i=1}^{batch\\_size} P_i $\n",
        "\n",
        "- Update the running total assignment and calculate the mean total assignment:\n",
        "\n",
        "$ \\text{running_total_assignment} \\ += step\\_total\\_assignment $\n",
        "\n",
        "$ \\text{mean_total_assignment} = \\frac{1}{num\\_experts} \\sum_{j=1}^{num\\_experts} \\text{running_total_assignment}_j $\n",
        "\n",
        "- Apply an expert mask based on the load balancing threshold:\n",
        "\n",
        "$$ \\text{expert_mask}_j = \\begin{cases}\n",
        "  0 & \\text{if } \\text{running_total_assignment}_j - \\text{mean_total_assignment} > threshold \\\\\n",
        "  1 & \\text{otherwise}\n",
        "\\end{cases} $$\n",
        "\n",
        "- Adjust expert weights and reapply softmax:\n",
        "\n",
        "$ P_{adjusted} = P \\times \\text{expert_mask} $\n",
        "\n",
        "$ P_{final} = \\text{Softmax}(P_{adjusted}) $\n",
        "\n",
        "### Summary\n",
        "\n",
        "The `GateLayerTopKLoadBalancing` class implements a gating mechanism that selects the top \\(k\\) experts based on modified gating scores, applies tunable noise for variability, and optionally balances the load among experts to prevent over-reliance on specific ones. This approach aims to enhance efficiency and performance by dynamically allocating computational resources in a Mixture of Experts model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gS3GSFmomLb"
      },
      "outputs": [],
      "source": [
        "# We select k experts here\n",
        "# We modify the gate\n",
        "\n",
        "class GateLayerTopKLoadBalancing(torch.nn.Module):\n",
        "    def __init__(self, in_features, hidden_size, num_experts, top_k: int, balance=True, threshold=0.1):\n",
        "        super(GateLayerTopKLoadBalancing, self).__init__()\n",
        "        self.top_k = top_k\n",
        "        self.in_features = in_features\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "        # Gate non linearity + weighting\n",
        "        self.layer1 = torch.nn.Linear(in_features, hidden_size)\n",
        "        self.softmax = torch.nn.Softmax(dim=1)\n",
        "        self.layer2 = torch.nn.Linear(hidden_size, num_experts)\n",
        "\n",
        "        self.balance=balance\n",
        "        self.threshold=x\n",
        "        self.running_total_assignment = torch.zeros(num_experts)\n",
        "\n",
        "        # Tunable noise\n",
        "        self.w_g = torch.nn.Parameter(torch.zeros(in_features, num_experts))\n",
        "        self.w_noise = torch.nn.Parameter(torch.zeros(in_features, num_experts))\n",
        "\n",
        "    def _get_tunable_noise(self, x):\n",
        "        \"\"\"Add noise to the input for load balancing. This will be elaborated later!\"\"\"\n",
        "        weighted_noise = torch.matmul(x, self.w_noise)\n",
        "        return torch.randn_like(weighted_noise) * weighted_noise\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        noise = self._get_tunable_noise(x)\n",
        "        self.running_total_assignment = self.running_total_assignment.to(x.device)\n",
        "        x = torch.nn.functional.relu(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        x = x + noise\n",
        "\n",
        "        # Addition\n",
        "        # We select bottom k experts and make them -infinity\n",
        "        k_bot = self.num_experts - self.top_k\n",
        "        _, bottom_k_indices = x.topk(k=k_bot, dim=1, largest=False)\n",
        "        x = x.scatter(dim=1, index=bottom_k_indices, value=-float(\"inf\"))\n",
        "        ## End of addition\n",
        "\n",
        "        expert_weights = self.softmax(x)\n",
        "\n",
        "        # Implement the load balancing\n",
        "        if self.balance:\n",
        "            # Add a load balancing mechanism here\n",
        "            # Calculate the assignment for each expert\n",
        "\n",
        "        return expert_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5q_c0uAomLb"
      },
      "outputs": [],
      "source": [
        "moemodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0HSN7zAomLb"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "gate2 = GateLayerTopKLoadBalancing(\n",
        "    in_features=784,\n",
        "    hidden_size=100,\n",
        "    num_experts=5,\n",
        "    top_k=2,\n",
        ")\n",
        "\n",
        "experts2 = generate_n_experts(\n",
        "    n=5,\n",
        "    in_features=784,\n",
        "    out_features=10\n",
        ")\n",
        "\n",
        "moe3 = MoELayer(\n",
        "    experts=experts2,\n",
        "    gate_layer=gate2,\n",
        "    is_sequence=False\n",
        ")\n",
        "\n",
        "moemodel3 = LitClassification(moe3)\n",
        "\n",
        "trainer = LightningTrainer(max_epochs=1)\n",
        "\n",
        "trainer.fit(moemodel3, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkQ8-X27omLc"
      },
      "outputs": [],
      "source": [
        "probe_result(trainer, moemodel3, test_loader, moe3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2u-36fMomLc"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz05ibrKomLc"
      },
      "source": [
        "## You get the idea how to implement MoE!\n",
        "\n",
        "Now, let's move on how to implement them to transformers.\n",
        "\n",
        "Basically, we can implement MoE in any layer. However,  research communities tend to implement MoE in intermediate layer of each self-attention layer.\n",
        "\n",
        "So, let's follow the trend :\\)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKEyAHL9omLc"
      },
      "source": [
        "We will inject MoE to a Bert-Tiny (Substantially mini BERT) to train a model for intent classification task using MASSIVE dataset.\n",
        "\n",
        "Implement if `is_sequence` is True!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUGOBjAHomLc"
      },
      "outputs": [],
      "source": [
        "def inject_moe(model, moe_target, moe_layer_cls, gate_layer_cls, num_experts = 5):\n",
        "  for moe_address in moe_target:\n",
        "    # Loop to get the module object\n",
        "    moe_address, is_sequence = moe_address\n",
        "    splitted_moe_address = moe_address.split(\".\")\n",
        "    parents = splitted_moe_address[:-1]\n",
        "    replaced_child = splitted_moe_address[-1]\n",
        "    parent_module = model\n",
        "    for parent in parents:\n",
        "      parent_module = getattr(parent_module, parent)\n",
        "\n",
        "    # we clone this weights\n",
        "    child = getattr(parent_module, replaced_child)\n",
        "    experts = [deepcopy(child) for i in range(num_experts)]\n",
        "    gate_layer = gate_layer_cls(child.in_features, hidden_size=128, num_experts=num_experts)\n",
        "    moe_layer = moe_layer_cls(experts, gate_layer, is_sequence )\n",
        "    print(f\"Replacing {moe_address}\")\n",
        "    setattr(parent_module, replaced_child, moe_layer)\n",
        "\n",
        "def tokenize(x, tokenizer):\n",
        "  out = tokenizer(x['utt'])\n",
        "  out['label'] = x['intent']\n",
        "  return out\n",
        "\n",
        "def get_f1(data, tokenizer, model):\n",
        "  dl = DataLoader(data['test'], collate_fn=DataCollatorWithPadding(tokenizer=tokenizer), batch_size=64)\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    predicted = []\n",
        "    targets = []\n",
        "    for x in dl:\n",
        "      batch = {i:j.cuda() for i,j in x.items()}\n",
        "      out = model(**batch)\n",
        "      out_pred = out.logits.argmax(dim=-1)\n",
        "      targets.extend(batch['labels'].cpu().detach().numpy())\n",
        "      predicted.extend(out_pred.cpu().detach().numpy())\n",
        "  return f1_score(predicted, targets, average='macro')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the Amazon MASSIVE English Intent Classification dataset with 60-class intent."
      ],
      "metadata": {
        "id": "DvnX0Aq6Yhgl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7ELSZ6ComLc"
      },
      "outputs": [],
      "source": [
        "data = load_dataset(\"AmazonScience/massive\", \"en-US\")\n",
        "num_label = len(set(data['train']['intent']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9H-eXQvomLc"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\", num_labels=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_cF17iEomLc"
      },
      "outputs": [],
      "source": [
        "moe_targets = [\n",
        "    # name layer, is_sequence\n",
        "  (\"bert.encoder.layer.0.output.dense\",True),\n",
        "  (\"bert.encoder.layer.1.output.dense\",True),\n",
        "  (\"bert.pooler.dense\", False),\n",
        "  (\"classifier\", False)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARUhMkt0omLc"
      },
      "outputs": [],
      "source": [
        "inject_moe(model, moe_targets, MoELayer, GateLayer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4c58Q8_omLc"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHlt7rg3omLc"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlBb6VXEomLc"
      },
      "outputs": [],
      "source": [
        "data = data.map(lambda x: tokenize(x, tokenizer), batched=True, remove_columns=data['train'].features.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzJ0EnF8omLc"
      },
      "outputs": [],
      "source": [
        "# Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./test-mbz-data\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=32,\n",
        "    save_total_limit=1,\n",
        "    learning_rate=1e-3,\n",
        "    evaluation_strategy=\"steps\",  # Perform evaluation at fixed steps\n",
        "    eval_steps=200,  # Number of steps to run evaluation (e.g., every 500 steps)\n",
        "    save_steps=200,\n",
        "    load_best_model_at_end=True,  # Optional: Load the best model at the end of training based on metric\n",
        "    metric_for_best_model=\"eval_loss\",  # Specify the metric to use for best model (if load_best_model_at_end is True)\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    train_dataset=data['train'],\n",
        "    eval_dataset=data['validation']\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_f1(data, tokenizer, model)"
      ],
      "metadata": {
        "id": "noLZfWaZevjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VORUaZNkomLj"
      },
      "source": [
        "\n",
        "Compare without MoE, how's your result?\n",
        "\n",
        "\n",
        "After all of above experiments, what is your conclusion?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZqeCEEIomLj"
      },
      "source": [
        "## Switch Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRgjTHBTomLj"
      },
      "source": [
        "This is the gating mechanism of Switch Transformers. This gating can be implemented to other model, however there's an additional loss that we should implement to maintain the load balancing. This might take times. For the sake of the time, we will just runthrough explaining the code.\n",
        "\n",
        "The code below is obtained through huggingface repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "289Z8gMfomLj"
      },
      "outputs": [],
      "source": [
        "class SwitchTransformersTop1Router(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Router using tokens choose top-1 experts assignment.\n",
        "\n",
        "    This router uses the same mechanism as in Switch Transformer (https://arxiv.org/abs/2101.03961) and V-MoE\n",
        "    (https://arxiv.org/abs/2106.05974): tokens choose their top experts. Items are sorted by router_probs and then\n",
        "    routed to their choice of expert until the expert's expert_capacity is reached. **There is no guarantee that each\n",
        "    token is processed by an expert**, or that each expert receives at least one token.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: SwitchTransformersConfig):\n",
        "        super().__init__()\n",
        "        self.num_experts = config.num_experts\n",
        "        self.expert_capacity = config.expert_capacity\n",
        "        self.classifier = torch.nn.Linear(config.hidden_size, self.num_experts, bias=config.router_bias)\n",
        "        self.jitter_noise = config.router_jitter_noise\n",
        "        self.ignore_padding_tokens = config.router_ignore_padding_tokens\n",
        "        self.dtype = getattr(torch, config.router_dtype)\n",
        "\n",
        "    def _compute_router_probabilities(self, hidden_states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        r\"\"\"\n",
        "        Computes router probabilities from input hidden states.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (`torch.Tensor`):\n",
        "                (batch_size, sequence_length, hidden_dim) from which router probabilities are computed.\n",
        "        Returns:\n",
        "            router_probabilities (`torch.Tensor`):\n",
        "                Tensor of shape (batch_size, sequence_length, num_experts) corresponding to the probabilities for each\n",
        "                token and expert. Used for routing tokens to experts.\n",
        "            router_logits (`torch.Tensor`):\n",
        "                Logits tensor of shape (batch_size, sequence_length, num_experts) corresponding to raw router logits.\n",
        "                This is used later for computing router z-loss.\n",
        "        \"\"\"\n",
        "        # We also store the previous dtype to cast back the output to the previous dtype\n",
        "        self.input_dtype = hidden_states.dtype\n",
        "        hidden_states = hidden_states.to(self.dtype)\n",
        "\n",
        "        if self.training and self.jitter_noise > 0:\n",
        "            # Multiply the token inputs by the uniform distribution - adding some noise\n",
        "            hidden_states *= torch.empty_like(hidden_states).uniform_(1.0 - self.jitter_noise, 1.0 + self.jitter_noise)\n",
        "\n",
        "        # Shape: [num_groups, tokens_per_group, num_experts]\n",
        "        self._cast_classifier()\n",
        "        router_logits = self.classifier(hidden_states)\n",
        "\n",
        "        # Apply Softmax and cast back to the original `dtype`\n",
        "        router_probabilities = torch.nn.functional.softmax(router_logits, dim=-1, dtype=self.dtype).to(self.input_dtype)\n",
        "        return router_probabilities, router_logits\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> Tuple:\n",
        "        r\"\"\"\n",
        "        Generic forward function for every Router class. Each Router expects to have the same input hidden states\n",
        "        (`hidden_states`) corresponding to the hidden states for each token, the `expert_capacity` corresponding to the\n",
        "        number of tokens the Router will send to each expert, some Routers can send up to few tokens to each expert.\n",
        "\n",
        "        Each Router works as the following: it expects the hidden states for each token, gets the `router_probs` and\n",
        "        `router_logits` from the `router_weights`. This will assign for each token, the raw probability to be assigned\n",
        "        to an expert. Then each Router class will have to define its own `_compute_routing_instructions`.\n",
        "\n",
        "        Args:\n",
        "            hidden_states (`torch.Tensor`) :\n",
        "                [num_groups, tokens_per_group, hidden_dim] inputs to send to experts.\n",
        "        Returns:\n",
        "            Tuple[`torch.Tensor`, `torch.Tensor`, `torch.Tensor`] Tuple containing the expert index, the router probs\n",
        "            and the router logits. The router probabilities and logits are required to compute the loss.\n",
        "        \"\"\"\n",
        "        router_probs, router_logits = self._compute_router_probabilities(hidden_states)\n",
        "\n",
        "        expert_index = torch.argmax(router_probs, dim=-1)\n",
        "        expert_index = torch.nn.functional.one_hot(expert_index, num_classes=self.num_experts)\n",
        "\n",
        "        # Mask tokens outside expert capacity. Sum over each sequence\n",
        "        token_priority = torch.cumsum(expert_index, dim=-2)\n",
        "        # mask if the token routed to to the expert will overflow\n",
        "        expert_capacity_mask = token_priority <= self.expert_capacity\n",
        "        expert_index = expert_index * expert_capacity_mask\n",
        "\n",
        "        router_probs = torch.max(router_probs, dim=-1).values.unsqueeze(-1)\n",
        "        return expert_index, router_probs, router_logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CODxcdVFomLj"
      },
      "source": [
        "After you understand the above code, feel free to run the switch transformers to train the MASSIVE dataset. Note that it does not contain SwitchTransformersForSequenceClassification. You can either use `SwitchTransformersForConditionalGeneration` or extract its encoder and do classification on top of it.\n",
        "\n",
        "Happy learning 😊😊\n",
        "\n",
        "## Challenge\n",
        "\n",
        "**NOTE: Hard difficulty, don't try if you don't have time. Nonetheless, doing this will improve your coding skill.**\n",
        "\n",
        "Implement Switch Transformers from scratch and try making the Q K and V layer, MoE!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKvpxetcomLj"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "7K1Zy7ZcomLZ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}