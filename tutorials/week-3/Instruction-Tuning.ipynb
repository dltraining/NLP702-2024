{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edeebe15-81dc-405e-93fc-d529bed75f13",
   "metadata": {},
   "source": [
    "## Instruction Tuning\n",
    "\n",
    "Supervised fine tuning (SFT) is fine-tuning all of a model’s parameters on supervised data of inputs and outputs. It teaches the model how to follow user specified instructions. It is typically done after model pre-training. **Source**: http://tinyurl.com/2v884put\n",
    "\n",
    "![instruction tuning](assets/instruction-tuning.jpg)\n",
    "\n",
    "Image Source: https://medium.com/mantisnlp/supervised-fine-tuning-customizing-llms-a2c1edbf22c3\n",
    "\n",
    "Requirement.\n",
    "1. Pre-trained model & tokenizer -> We will get it from huggingface.\n",
    "2. Instruction-Response pair data -> eg: Alpaca, Dolly, Oasst1, LIMA, etc. We will get the dataset from huggingface. \n",
    "\n",
    "Steps. \n",
    "1. Load pre-trained model and tokenizer.\n",
    "2. Format the instructions response pair.\n",
    "3. Preprocess the dataset.\n",
    "4. Train the pre-trained model in supervised setting with response as labels and instruction as input.\n",
    "5. Evaluation:\n",
    "   i. Automatic Evaluation: Eg: MMLU, BBH, AGIEval, domain-specific evaluation such as maths, reasoning, code.\n",
    "   ii. Human Evaluation: Give model prompts to generate a response and ask humans.\n",
    "   iii. LLM as Evaluator: Ask powerful models such as GPT4 to rate the response generated by the your finetuned model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e36884d-9716-4f46-8259-74571b90def1",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b31d4796-4d5a-40bc-b7cf-561133d4233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer\n",
    "from datasets import load_dataset\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adabbac-561f-4251-9bfd-ad1d92f6849f",
   "metadata": {},
   "source": [
    "##### 1. Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d5dbf5-b851-4307-85f4-eea519349249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fe7330c-2c92-47d7-8f70-fdbd86beaeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"microsoft/phi-1_5\" # huggingface model name \n",
    "dataset_name_or_path = \"xzuyn/lima-alpaca\" # LIMA Data in Vicuna Format. https://arxiv.org/abs/2305.11206\n",
    "split_name=\"train\"\n",
    "inst_col_name=\"instruction\"\n",
    "input_col_name=\"input\"\n",
    "output_col_name=\"output\"\n",
    "model_max_length=512 # how long sequence model can process\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7a66d9-312e-4c31-8f08-bda03acede6a",
   "metadata": {},
   "source": [
    "##### 2. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19cc1420-c916-44f2-9262-a34e3ca982f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a3e555-b5bf-43d8-bebb-ff5863fa9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "- This is two prompt template / or wrapper we are going to use. \n",
    "- Some instruction contains \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12ce29f6-1938-42ae-a77e-7ad8e52c3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\":a (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cd86f44-f95a-4e90-a488-60350eccf2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f729a17-a9ca-4e53-9b4c-196834bfe5e1",
   "metadata": {},
   "source": [
    "If we are adding any new tokens to then we need to extend the embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce8b518d-d9dc-4cd8-ac37-24641e0eedd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75946c4e-28c9-4ed3-ad63-56bb9c4cc45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dcabd11-3dbe-41f1-9f90-f86fdd31ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb0f0280-68e5-44f5-8494-71d793a66c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c062b8e5-631c-4a06-9859-8f9b985127af",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = [dataset[split_name][inst_col_name][0]]\n",
    "target = [dataset[split_name][output_col_name][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76742280-f2cc-41bc-a8b1-c6e7be1fd8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = preprocess(sources=source, targets=target, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "807457e4-77ec-43b8-b1d4-15777089fc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3a2ccca-33c8-430b-a4c6-fedd3f01dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(outputs['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2666a9bf-8fc4-4532-8293-bc581e34cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ff8d308-0c54-4018-b6c4-adb767b53b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_name_or_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "\n",
    "        # Load the dataset\n",
    "        logging.warning(\"Loading data...\")\n",
    "        dataset = datasets.load_dataset(dataset_name_or_path, split=split_name)\n",
    "        \n",
    "        \n",
    "        logging.warning(\"Formatting inputs...\")\n",
    "        # if there is no input for prompt the use prompt_no_input template else use prompt_input template\n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "        sources = [\n",
    "            prompt_input.format_map(example) if example.get(input_col_name, \"\") != \"\" else prompt_no_input.format_map(example)\n",
    "            for example in tqdm(dataset)\n",
    "        ]\n",
    "        targets = [f\"{example[output_col_name]}{tokenizer.eos_token}\" for example in dataset]\n",
    "\n",
    "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "        data_dict = preprocess(sources, targets, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "164bd107-ad15-4951-beb0-47442a072be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example in dataset['train']:\n",
    "#     print(example.get(input_col_name, \"input\"))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7e0e317-c18e-450e-bb54-666150c17c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 33024.72it/s]\u001b[A\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    }
   ],
   "source": [
    "dataset = SupervisedDataset(dataset_name_or_path=dataset_name_or_path, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5009b995-5b48-4008-a1aa-7b147511fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf3e2ea8-9fc1-4513-86cf-77be5a120243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(dataset[0]['labels'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "03ccc829-c75e-4fe4-9abc-e5f83cfe9183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f8190af7-54eb-4d86-979b-0a5de8ae7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_args) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path, debugging=data_args.debugging)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a57c9dde-4590-4dbe-9b48-0e6663cfe46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_agrs):\n",
    "\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path=model_name_or_path,\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "    model.config.use_cache=False\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path=model_name_or_path,\n",
    "        cache_dir=cache_dir,\n",
    "        model_max_length=model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        smart_tokenizer_and_embedding_resize(\n",
    "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "        )\n",
    "    if \"llama\" in model_args.model_name_or_path:\n",
    "        tokenizer.add_special_tokens(\n",
    "            {\n",
    "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
    "    \n",
    "    # update training args to make output dir\n",
    "    output_dir = os.path.join(training_args.output_dir, model_name_or_path.split(\"/\")[-1])\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    training_args.output_dir = output_dir\n",
    "    \n",
    "    trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)\n",
    "    \n",
    "    # resume from last checkpoint if it exists     \n",
    "    checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "\n",
    "    if checkpoint:\n",
    "        print(f\"Checkpoint found! Training from {checkpoint} checkpoint!\")\n",
    "        trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    else:\n",
    "        print(f\"No checkpoint found! Training from scratch!\")\n",
    "        trainer.train()\n",
    "    \n",
    "    # trainer.train()\n",
    "    # save states \n",
    "    trainer.save_state()\n",
    "    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n",
    "    print(f\"Training finished! Saved model to {training_args.output_dir}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28933e42-bd69-4939-84e3-8a40ee361b89",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd25061-8c21-4f20-9dc8-cfd20503e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48577776-fdcb-4bed-ad61-4c7e79d774a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(output_dir=\"./output_dir\", training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db9e526-ce4f-45c5-9fa8-70563a953a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args.num_train_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44feac4b-8b5b-412b-b5de-991adf5f18f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "327f9a00-cad9-4fc9-bd5d-22738573f34b",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3182e47-0897-41ae-821e-1b3b74c512a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone llm-evaluation-harness\n",
    "git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
    "cd lm-evaluation-harness\n",
    "pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d67e2-0bd8-4afa-8a38-69d39401383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=EleutherAI/pythia-160m,revision=step100000,dtype=\"float\" \\\n",
    "    --tasks lambada_openai,hellaswag \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size auto:4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eadf1f-7b96-4799-a550-d166ff487d76",
   "metadata": {},
   "source": [
    "### Prompting\n",
    "- Once we have trained the model to follow instructions, we can prompt that model to generate a response.\n",
    "- We will be using HF's generation pipeline to prompt our trained model.\n",
    "- After training the model with a particular prompt wrapper it is advised to use the same prompt format during inference.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef69ec5-8f1d-4bcb-a988-987a853ec9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64253dce-fe52-42c5-abd2-3f84a7f70b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and trained model and then create chatbot pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-13b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda:0\")\n",
    "\n",
    "chatbot = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea89e1a2-7c57-4006-8cf4-426199a0318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = chatbot(\n",
    "    input_prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.4,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=1024,\n",
    "    return_full_text=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecf3e3f-4553-44df-b00c-318fec4ae96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequences[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c04e1-d302-4f2e-8bce-ae0f26f03d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53f136b1-574a-46ff-8ade-5565db02229e",
   "metadata": {},
   "source": [
    "#### Next Tutorial - Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0a0fe-4416-481b-8d03-b9cb89889204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
