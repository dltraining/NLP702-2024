{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "edeebe15-81dc-405e-93fc-d529bed75f13",
      "metadata": {
        "id": "edeebe15-81dc-405e-93fc-d529bed75f13"
      },
      "source": [
        "## Instruction Tuning\n",
        "\n",
        "Supervised fine tuning (SFT) is fine-tuning all of a model’s parameters on supervised data of inputs and outputs. It teaches the model how to follow user specified instructions. It is typically done after model pre-training. **Source**: http://tinyurl.com/2v884put\n",
        "\n",
        "![instruction tuning](assets/instruction-tuning.jpg)\n",
        "\n",
        "Image Source: https://medium.com/mantisnlp/supervised-fine-tuning-customizing-llms-a2c1edbf22c3\n",
        "\n",
        "Requirement.\n",
        "1. Pre-trained model & tokenizer -> We will get it from huggingface.\n",
        "2. Instruction-Response pair data -> eg: Alpaca, Dolly, Oasst1, LIMA, etc. We will get the dataset from huggingface.\n",
        "\n",
        "Steps.\n",
        "1. Load pre-trained model and tokenizer.\n",
        "2. Format the instructions response pair.\n",
        "3. Preprocess the dataset.\n",
        "4. Train the pre-trained model in supervised setting with response as labels and instruction as input.\n",
        "5. Evaluation:\n",
        "   i. Automatic Evaluation: Eg: MMLU, BBH, AGIEval, domain-specific evaluation such as maths, reasoning, code.\n",
        "   ii. Human Evaluation: Give model prompts to generate a response and ask humans.\n",
        "   iii. LLM as Evaluator: Ask powerful models such as GPT4 to rate the response generated by the your finetuned model.\n",
        "\n",
        "\n",
        "  \n",
        "**Note**: Most of the code were borrowed from original alpaca code here: https://github.com/tatsu-lab/stanford_alpaca/tree/main"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e36884d-9716-4f46-8259-74571b90def1",
      "metadata": {
        "id": "4e36884d-9716-4f46-8259-74571b90def1"
      },
      "source": [
        "#### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install --quiet datasets accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isqPaGANPygQ",
        "outputId": "9bccba6e-40c7-411e-ae45-a2331fabbc13"
      },
      "id": "isqPaGANPygQ",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.35.2\n",
            "Uninstalling transformers-4.35.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/transformers-cli\n",
            "    /usr/local/lib/python3.10/dist-packages/transformers-4.35.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/transformers/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled transformers-4.35.2\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-ed0w6hr7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-ed0w6hr7\n",
            "  Resolved https://github.com/huggingface/transformers to commit 5f81266fb0b444f896fea9322d7c41368abd8526\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0.dev0) (2023.11.17)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.38.0.dev0-py3-none-any.whl size=8408053 sha256=eddd9ae81ad40b6b6796ca559cdf920553ac44101f1df112e27fb4831b10fca2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uh7i7_m1/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.38.0.dev0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b31d4796-4d5a-40bc-b7cf-561133d4233b",
      "metadata": {
        "id": "b31d4796-4d5a-40bc-b7cf-561133d4233b"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "import os\n",
        "import copy\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Sequence\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import datasets\n",
        "import transformers\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer\n",
        "from datasets import load_dataset\n",
        "from transformers.trainer_utils import get_last_checkpoint, is_main_process"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5adabbac-561f-4251-9bfd-ad1d92f6849f",
      "metadata": {
        "id": "5adabbac-561f-4251-9bfd-ad1d92f6849f"
      },
      "source": [
        "##### 1. Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "13d5dbf5-b851-4307-85f4-eea519349249",
      "metadata": {
        "id": "13d5dbf5-b851-4307-85f4-eea519349249"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6fe7330c-2c92-47d7-8f70-fdbd86beaeb3",
      "metadata": {
        "id": "6fe7330c-2c92-47d7-8f70-fdbd86beaeb3"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"gpt2\" #\"microsoft/phi-1_5\" -> use \"gpt2\" if you have less powerful GPU # huggingface model name\n",
        "dataset_name_or_path = \"xzuyn/lima-alpaca\" # LIMA Data in Vicuna Format. https://arxiv.org/abs/2305.11206\n",
        "cache_dir=\"cache_dir\"\n",
        "split_name=\"train\"\n",
        "inst_col_name=\"instruction\"\n",
        "input_col_name=\"input\"\n",
        "output_col_name=\"output\"\n",
        "model_max_length=512 # how long sequence model can process\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b7a66d9-312e-4c31-8f08-bda03acede6a",
      "metadata": {
        "id": "5b7a66d9-312e-4c31-8f08-bda03acede6a"
      },
      "source": [
        "##### 2. Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "19cc1420-c916-44f2-9262-a34e3ca982f6",
      "metadata": {
        "id": "19cc1420-c916-44f2-9262-a34e3ca982f6"
      },
      "outputs": [],
      "source": [
        "# dataset = load_dataset(dataset_name_or_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This is two prompt template / or wrapper we are going to use.\n",
        "- Some instruction contains\n"
      ],
      "metadata": {
        "id": "oX-KNklNzxrr"
      },
      "id": "oX-KNklNzxrr"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "12ce29f6-1938-42ae-a77e-7ad8e52c3306",
      "metadata": {
        "id": "12ce29f6-1938-42ae-a77e-7ad8e52c3306"
      },
      "outputs": [],
      "source": [
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
        "    ),\n",
        "    \"prompt_no_input\":(\n",
        "        \"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
        "    ),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1cd86f44-f95a-4e90-a488-60350eccf2fe",
      "metadata": {
        "id": "1cd86f44-f95a-4e90-a488-60350eccf2fe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8f729a17-a9ca-4e53-9b4c-196834bfe5e1",
      "metadata": {
        "id": "8f729a17-a9ca-4e53-9b4c-196834bfe5e1"
      },
      "source": [
        "If we are adding any new tokens to then we need to extend the embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ce8b518d-d9dc-4cd8-ac37-24641e0eedd0",
      "metadata": {
        "id": "ce8b518d-d9dc-4cd8-ac37-24641e0eedd0"
      },
      "outputs": [],
      "source": [
        "def smart_tokenizer_and_embedding_resize(\n",
        "    special_tokens_dict: Dict,\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        "    model: transformers.PreTrainedModel,\n",
        "):\n",
        "    \"\"\"Resize tokenizer and embedding.\n",
        "\n",
        "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
        "    \"\"\"\n",
        "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if num_new_tokens > 0:\n",
        "        input_embeddings = model.get_input_embeddings().weight.data\n",
        "        output_embeddings = model.get_output_embeddings().weight.data\n",
        "\n",
        "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "\n",
        "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
        "        output_embeddings[-num_new_tokens:] = output_embeddings_avg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize the data."
      ],
      "metadata": {
        "id": "-B7AIm2BvoZy"
      },
      "id": "-B7AIm2BvoZy"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "75946c4e-28c9-4ed3-ad63-56bb9c4cc45c",
      "metadata": {
        "id": "75946c4e-28c9-4ed3-ad63-56bb9c4cc45c"
      },
      "outputs": [],
      "source": [
        "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "    \"\"\"Tokenize a list of strings.\"\"\"\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2dcabd11-3dbe-41f1-9f90-f86fdd31ccd8",
      "metadata": {
        "id": "2dcabd11-3dbe-41f1-9f90-f86fdd31ccd8"
      },
      "outputs": [],
      "source": [
        "def preprocess(\n",
        "    sources: Sequence[str],\n",
        "    targets: Sequence[str],\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        ") -> Dict:\n",
        "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5ff8d308-0c54-4018-b6c4-adb767b53b50",
      "metadata": {
        "id": "5ff8d308-0c54-4018-b6c4-adb767b53b50"
      },
      "outputs": [],
      "source": [
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset_name_or_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
        "        super(SupervisedDataset, self).__init__()\n",
        "\n",
        "        # Load the dataset\n",
        "        logging.warning(\"Loading data...\")\n",
        "        dataset = datasets.load_dataset(dataset_name_or_path, split=split_name)\n",
        "\n",
        "\n",
        "        logging.warning(\"Formatting inputs...\")\n",
        "        # if there is no input for prompt the use prompt_no_input template else use prompt_input template\n",
        "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
        "        sources = [\n",
        "            prompt_input.format_map(example) if example.get(input_col_name, \"\") != \"\" else prompt_no_input.format_map(example)\n",
        "            for example in tqdm(dataset)\n",
        "        ]\n",
        "        targets = [f\"{example[output_col_name]}{tokenizer.eos_token}\" for example in dataset]\n",
        "\n",
        "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
        "        data_dict = preprocess(sources, targets, tokenizer)\n",
        "\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "164bd107-ad15-4951-beb0-47442a072be7",
      "metadata": {
        "id": "164bd107-ad15-4951-beb0-47442a072be7"
      },
      "outputs": [],
      "source": [
        "# for example in dataset['train']:\n",
        "#     print(example.get(input_col_name, \"input\"))\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f7e0e317-c18e-450e-bb54-666150c17c5e",
      "metadata": {
        "id": "f7e0e317-c18e-450e-bb54-666150c17c5e"
      },
      "outputs": [],
      "source": [
        "# dataset = SupervisedDataset(dataset_name_or_path=dataset_name_or_path, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )"
      ],
      "metadata": {
        "id": "L-lgnXWf0lRD"
      },
      "id": "L-lgnXWf0lRD",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dLTcAZUU0ueb"
      },
      "id": "dLTcAZUU0ueb",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f8190af7-54eb-4d86-979b-0a5de8ae7f6e",
      "metadata": {
        "id": "f8190af7-54eb-4d86-979b-0a5de8ae7f6e"
      },
      "outputs": [],
      "source": [
        "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
        "    train_dataset = SupervisedDataset(tokenizer=tokenizer, dataset_name_or_path=dataset_name_or_path)\n",
        "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
        "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
        "    state_dict = trainer.model.state_dict()\n",
        "    if trainer.args.should_save:\n",
        "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
        "        del state_dict\n",
        "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa"
      ],
      "metadata": {
        "id": "hvi5uWBa1emE"
      },
      "id": "hvi5uWBa1emE",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a57c9dde-4590-4dbe-9b48-0e6663cfe46c",
      "metadata": {
        "id": "a57c9dde-4590-4dbe-9b48-0e6663cfe46c"
      },
      "outputs": [],
      "source": [
        "def train(training_args):\n",
        "\n",
        "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        cache_dir=cache_dir,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    model.config.use_cache=False\n",
        "\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "        model_name_or_path,\n",
        "        cache_dir=cache_dir,\n",
        "        model_max_length=model_max_length,\n",
        "        padding_side=\"right\",\n",
        "        use_fast=False,\n",
        "    )\n",
        "    if tokenizer.pad_token is None:\n",
        "        smart_tokenizer_and_embedding_resize(\n",
        "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
        "            tokenizer=tokenizer,\n",
        "            model=model,\n",
        "        )\n",
        "    if \"llama\" in model_name_or_path:\n",
        "        tokenizer.add_special_tokens(\n",
        "            {\n",
        "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
        "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
        "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    data_module = make_supervised_data_module(tokenizer=tokenizer)\n",
        "\n",
        "    # update training args to make output dir\n",
        "    output_dir = os.path.join(training_args.output_dir, model_name_or_path.split(\"/\")[-1])\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    training_args.output_dir = output_dir\n",
        "\n",
        "    trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)\n",
        "\n",
        "    # resume from last checkpoint if it exists\n",
        "    checkpoint = get_last_checkpoint(training_args.output_dir)\n",
        "\n",
        "    if checkpoint:\n",
        "        print(f\"Checkpoint found! Training from {checkpoint} checkpoint!\")\n",
        "        trainer.train(resume_from_checkpoint=checkpoint)\n",
        "    else:\n",
        "        print(f\"No checkpoint found! Training from scratch!\")\n",
        "        trainer.train()\n",
        "\n",
        "    # trainer.train()\n",
        "    # save states\n",
        "    trainer.save_state()\n",
        "    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n",
        "    print(f\"Training finished! Saved model to {training_args.output_dir}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28933e42-bd69-4939-84e3-8a40ee361b89",
      "metadata": {
        "id": "28933e42-bd69-4939-84e3-8a40ee361b89"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"output\" #"
      ],
      "metadata": {
        "id": "aBHgmBOO1VWD"
      },
      "id": "aBHgmBOO1VWD",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = transformers.TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    num_train_epochs=2,\n",
        "    logging_steps=10,\n",
        ")"
      ],
      "metadata": {
        "id": "PuMQtT6U1Qt1"
      },
      "id": "PuMQtT6U1Qt1",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformers.TrainingArguments?"
      ],
      "metadata": {
        "id": "RiJIyyfx51eq"
      },
      "id": "RiJIyyfx51eq",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "7bd25061-8c21-4f20-9dc8-cfd20503e3fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "7bd25061-8c21-4f20-9dc8-cfd20503e3fa",
        "outputId": "fad32511-360f-4aa1-c62a-520691a84e0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:root:Loading data...\n",
            "WARNING:root:Formatting inputs...\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 26336.37it/s]\n",
            "WARNING:root:Tokenizing inputs... This may take some time...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoint found! Training from scratch!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='124' max='124' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [124/124 04:38, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.083900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.079000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.063200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.130600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.056400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>3.049100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.991600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.910600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.975100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.953300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.802100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>2.881600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished! Saved model to output/gpt2.\n"
          ]
        }
      ],
      "source": [
        "# training\n",
        "train(training_args=training_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DePLcLnOWiGQ"
      },
      "id": "DePLcLnOWiGQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44feac4b-8b5b-412b-b5de-991adf5f18f6",
      "metadata": {
        "id": "44feac4b-8b5b-412b-b5de-991adf5f18f6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "327f9a00-cad9-4fc9-bd5d-22738573f34b",
      "metadata": {
        "id": "327f9a00-cad9-4fc9-bd5d-22738573f34b"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls lm-evaluation-harness/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM_fAT-k7Oug",
        "outputId": "0459b897-4209-4c87-d684-e5006052dc52"
      },
      "id": "OM_fAT-k7Oug",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CITATION.bib  \u001b[0m\u001b[01;34mexamples\u001b[0m/   \u001b[01;34mlm_eval\u001b[0m/           pile_statistics.json  requirements.txt  \u001b[01;34mtemplates\u001b[0m/\n",
            "CODEOWNERS    ignore.txt  \u001b[01;34mlm_eval.egg-info\u001b[0m/  pyproject.toml        \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdocs\u001b[0m/         LICENSE.md  mypy.ini           README.md             setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "c3182e47-0897-41ae-821e-1b3b74c512a5",
      "metadata": {
        "id": "c3182e47-0897-41ae-821e-1b3b74c512a5"
      },
      "outputs": [],
      "source": [
        "# clone llm-evaluation-harness\n",
        "# !git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
        "# cd lm-evaluation-harness\n",
        "# !pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XggnQZFSWpjl"
      },
      "id": "XggnQZFSWpjl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Zero-shot model"
      ],
      "metadata": {
        "id": "SJIcaiXH2sME"
      },
      "id": "SJIcaiXH2sME"
    },
    {
      "cell_type": "code",
      "source": [
        "# zero-shot pre-trained model\n",
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=\"gpt2\" \\\n",
        "    --tasks sst2 \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto:4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5BxFT2Q2q9R",
        "outputId": "a195817e-3c7b-4e93-b39c-8eae56d9cf8f"
      },
      "id": "d5BxFT2Q2q9R",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-24:05:24:58,724 INFO     [utils.py:160] NumExpr defaulting to 2 threads.\n",
            "2024-01-24:05:24:58,987 INFO     [config.py:58] PyTorch version 2.1.0+cu121 available.\n",
            "2024-01-24:05:24:58,988 INFO     [config.py:95] TensorFlow version 2.15.0 available.\n",
            "2024-01-24:05:24:58,989 INFO     [config.py:108] JAX version 0.4.23 available.\n",
            "2024-01-24 05:24:59.465533: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-24 05:24:59.465590: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-24 05:24:59.466821: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-24 05:25:00.661867: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-24:05:25:04,183 INFO     [__main__.py:156] Verbosity set to INFO\n",
            "2024-01-24:05:25:08,391 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.\n",
            "2024-01-24:05:25:12,437 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.\n",
            "2024-01-24:05:25:12,438 INFO     [__main__.py:229] Selected Tasks: ['sst2']\n",
            "2024-01-24:05:25:12,452 INFO     [huggingface.py:148] Using device 'cuda:0'\n",
            "2024-01-24:05:25:14,506 WARNING  [task.py:603] [Task: sst2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
            "2024-01-24:05:25:14,506 WARNING  [task.py:615] [Task: sst2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
            "2024-01-24:05:25:17,976 INFO     [task.py:340] Building contexts for task on rank 0...\n",
            "2024-01-24:05:25:18,436 INFO     [evaluator.py:319] Running loglikelihood requests\n",
            "  0% 0/1744 [00:00<?, ?it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 64\n",
            " 18% 321/1744 [00:04<00:10, 132.87it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 64\n",
            "100% 1744/1744 [00:10<00:00, 167.90it/s]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "hf (pretrained=gpt2), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto:4 (64,64,64,64)\n",
            "|Tasks|Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
            "|-----|------:|------|-----:|------|-----:|---|-----:|\n",
            "|sst2 |      1|none  |     0|acc   |0.5505|±  |0.0169|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "de5d67e2-0bd8-4afa-8a38-69d39401383b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de5d67e2-0bd8-4afa-8a38-69d39401383b",
        "outputId": "93b1d16e-29df-474f-96d8-8b886e90c10d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-24:05:25:51,870 INFO     [utils.py:160] NumExpr defaulting to 2 threads.\n",
            "2024-01-24:05:25:52,137 INFO     [config.py:58] PyTorch version 2.1.0+cu121 available.\n",
            "2024-01-24:05:25:52,138 INFO     [config.py:95] TensorFlow version 2.15.0 available.\n",
            "2024-01-24:05:25:52,139 INFO     [config.py:108] JAX version 0.4.23 available.\n",
            "2024-01-24 05:25:52.615154: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-24 05:25:52.615202: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-24 05:25:52.616432: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-24 05:25:53.710163: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-24:05:25:56,950 INFO     [__main__.py:156] Verbosity set to INFO\n",
            "2024-01-24:05:26:02,001 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.\n",
            "2024-01-24:05:26:05,933 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.\n",
            "2024-01-24:05:26:05,934 INFO     [__main__.py:229] Selected Tasks: ['sst2']\n",
            "2024-01-24:05:26:05,943 INFO     [huggingface.py:148] Using device 'cuda:0'\n",
            "2024-01-24:05:26:06,782 WARNING  [task.py:603] [Task: sst2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
            "2024-01-24:05:26:06,782 WARNING  [task.py:615] [Task: sst2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
            "2024-01-24:05:26:10,511 INFO     [task.py:340] Building contexts for task on rank 0...\n",
            "2024-01-24:05:26:11,180 INFO     [evaluator.py:319] Running loglikelihood requests\n",
            "  0% 0/1744 [00:00<?, ?it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 64\n",
            " 18% 321/1744 [00:04<00:10, 130.42it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
            "Determined largest batch size: 64\n",
            "100% 1744/1744 [00:10<00:00, 162.45it/s]\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "hf (pretrained=/content/output/gpt2), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto:4 (64,64,64,64)\n",
            "|Tasks|Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
            "|-----|------:|------|-----:|------|-----:|---|-----:|\n",
            "|sst2 |      1|none  |     0|acc   |0.5998|±  |0.0166|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=/content/output/gpt2 \\\n",
        "    --tasks sst2 \\\n",
        "    --device cuda:0 \\\n",
        "    --batch_size auto:4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05eadf1f-7b96-4799-a550-d166ff487d76",
      "metadata": {
        "id": "05eadf1f-7b96-4799-a550-d166ff487d76"
      },
      "source": [
        "### Prompting\n",
        "- Once we have trained the model to follow instructions, we can prompt that model to generate a response.\n",
        "- We will be using HF's generation pipeline to prompt our trained model.\n",
        "- After training the model with a particular prompt wrapper it is advised to use the same prompt format during inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "bef69ec5-8f1d-4bcb-a988-987a853ec9e1",
      "metadata": {
        "id": "bef69ec5-8f1d-4bcb-a988-987a853ec9e1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "64253dce-fe52-42c5-abd2-3f84a7f70b0f",
      "metadata": {
        "id": "64253dce-fe52-42c5-abd2-3f84a7f70b0f"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and trained model and then create chatbot pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/output/gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"/content/output/gpt2\", device_map=\"cuda:0\")\n",
        "\n",
        "chatbot = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip show torch"
      ],
      "metadata": {
        "id": "ZJiy2iFW_0Tz"
      },
      "id": "ZJiy2iFW_0Tz",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# format the prompt\n",
        "text = \"What is Machine Learning?\"\n",
        "\n",
        "prompt = PROMPT_DICT['prompt_no_input'].format(instruction=text)"
      ],
      "metadata": {
        "id": "EhL-B70DXjS1"
      },
      "id": "EhL-B70DXjS1",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "iUYctoAK79Wq",
        "outputId": "9a6ef518-d7f6-42d8-ff65-04b1e6ec5209"
      },
      "id": "iUYctoAK79Wq",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is Machine Learning?\\n\\n### Response:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t3Ma2Y_-YR5A"
      },
      "id": "t3Ma2Y_-YR5A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ea89e1a2-7c57-4006-8cf4-426199a0318a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea89e1a2-7c57-4006-8cf4-426199a0318a",
        "outputId": "10cb5390-b5e2-415b-d82e-cfef9bd9ab6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "sequences = chatbot(\n",
        "    prompt,\n",
        "    do_sample=True,\n",
        "    temperature=0.1,\n",
        "    top_p=0.4,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    max_new_tokens=64,\n",
        "    return_full_text=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ps ux"
      ],
      "metadata": {
        "id": "Qg79HroWZe7Y"
      },
      "id": "Qg79HroWZe7Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "6ecf3e3f-4553-44df-b00c-318fec4ae96a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ecf3e3f-4553-44df-b00c-318fec4ae96a",
        "outputId": "81c18e1f-3e52-4044-9297-e18d90a5c713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning is a new field of research that has been gaining momentum in recent years. Machine learning is a new field of research that has been gaining momentum in recent years. Machine learning is a new field of research that has been gaining momentum in recent years.\n",
            "\n",
            "Machine learning is a new field of research that has been\n"
          ]
        }
      ],
      "source": [
        "print(sequences[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "733c04e1-d302-4f2e-8bce-ae0f26f03d26",
      "metadata": {
        "id": "733c04e1-d302-4f2e-8bce-ae0f26f03d26"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "53f136b1-574a-46ff-8ade-5565db02229e",
      "metadata": {
        "id": "53f136b1-574a-46ff-8ade-5565db02229e"
      },
      "source": [
        "#### Next Tutorial - Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01e0a0fe-4416-481b-8d03-b9cb89889204",
      "metadata": {
        "id": "01e0a0fe-4416-481b-8d03-b9cb89889204"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}