{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edeebe15-81dc-405e-93fc-d529bed75f13",
   "metadata": {
    "id": "edeebe15-81dc-405e-93fc-d529bed75f13"
   },
   "source": [
    "## Instruction Tuning\n",
    "\n",
    "Supervised fine tuning (SFT) is fine-tuning all of a model’s parameters on supervised data of inputs and outputs. It teaches the model how to follow user specified instructions. It is typically done after model pre-training. **Source**: http://tinyurl.com/2v884put\n",
    "\n",
    "![instruction tuning](assets/instruction-tuning.jpg)\n",
    "\n",
    "Image Source: https://medium.com/mantisnlp/supervised-fine-tuning-customizing-llms-a2c1edbf22c3\n",
    "\n",
    "Requirement.\n",
    "1. Pre-trained model & tokenizer -> We will get it from huggingface.\n",
    "2. Instruction-Response pair data -> eg: Alpaca, Dolly, Oasst1, LIMA, etc. We will get the dataset from huggingface.\n",
    "\n",
    "Steps.\n",
    "1. Load pre-trained model and tokenizer.\n",
    "2. Format the instructions response pair.\n",
    "3. Preprocess the dataset.\n",
    "4. Train the pre-trained model in supervised setting with response as labels and instruction as input.\n",
    "5. Evaluation:\n",
    "   i. Automatic Evaluation: Eg: MMLU, BBH, AGIEval, domain-specific evaluation such as maths, reasoning, code.\n",
    "   ii. Human Evaluation: Give model prompts to generate a response and ask humans.\n",
    "   iii. LLM as Evaluator: Ask powerful models such as GPT4 to rate the response generated by the your finetuned model.\n",
    "\n",
    "\n",
    "  \n",
    "**Note**: Most of the code were borrowed from original alpaca code here: https://github.com/tatsu-lab/stanford_alpaca/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e36884d-9716-4f46-8259-74571b90def1",
   "metadata": {
    "id": "4e36884d-9716-4f46-8259-74571b90def1"
   },
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "isqPaGANPygQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "isqPaGANPygQ",
    "outputId": "9bccba6e-40c7-411e-ae45-a2331fabbc13"
   },
   "outputs": [],
   "source": [
    "# !pip uninstall transformers\n",
    "# !pip install git+https://github.com/huggingface/transformers\n",
    "# !pip install --quiet datasets accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b31d4796-4d5a-40bc-b7cf-561133d4233b",
   "metadata": {
    "id": "b31d4796-4d5a-40bc-b7cf-561133d4233b"
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import os\n",
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer\n",
    "from datasets import load_dataset\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adabbac-561f-4251-9bfd-ad1d92f6849f",
   "metadata": {
    "id": "5adabbac-561f-4251-9bfd-ad1d92f6849f"
   },
   "source": [
    "##### 1. Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d5dbf5-b851-4307-85f4-eea519349249",
   "metadata": {
    "id": "13d5dbf5-b851-4307-85f4-eea519349249"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6fe7330c-2c92-47d7-8f70-fdbd86beaeb3",
   "metadata": {
    "id": "6fe7330c-2c92-47d7-8f70-fdbd86beaeb3"
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"UBC-NLP/Jasmine-350M\" #\"microsoft/phi-1_5\" -> use \"gpt2\" if you have less powerful GPU # huggingface model name\n",
    "dataset_name_or_path = \"macabdul9/NADI-Sample\" # LIMA Data in Vicuna Format. https://arxiv.org/abs/2305.11206\n",
    "cache_dir=\"cache_dir\"\n",
    "split_name=\"train\"\n",
    "inst_col_name=\"instruction\"\n",
    "input_col_name=\"source_dialect\"\n",
    "output_col_name=\"target_msa\"\n",
    "model_max_length=512 # how long sequence model can process\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7a66d9-312e-4c31-8f08-bda03acede6a",
   "metadata": {
    "id": "5b7a66d9-312e-4c31-8f08-bda03acede6a"
   },
   "source": [
    "##### 2. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "19cc1420-c916-44f2-9262-a34e3ca982f6",
   "metadata": {
    "id": "19cc1420-c916-44f2-9262-a34e3ca982f6"
   },
   "outputs": [],
   "source": [
    "# # dataset = load_dataset(dataset_name_or_path)\n",
    "# # Download the NADI dataset\n",
    "# #Donwload FT code and sample for subtask1\n",
    "# !wget https://raw.githubusercontent.com/UBC-NLP/nadi/main/run_NADI2023_country_level.py /content/run_NADI2023_country_level.py\n",
    "# !wget https://raw.githubusercontent.com/UBC-NLP/nadi/main/NADI2023_subtast1_sample.tsv NADI2023_subtast1_sample.tsv\n",
    "# #FT code and sample for subtask2 and subtask3\n",
    "# !wget https://raw.githubusercontent.com/UBC-NLP/nadi/main/run_NADI2023_MT.py /content/run_NADI2023_MT.py\n",
    "# !wget https://raw.githubusercontent.com/UBC-NLP/nadi/main/NADI2023_MT_examples.tsv /content/NADI2023_MT_examples.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "020504f2-1f3d-4ec6-aea0-de639f863405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ad8ce0c-14f1-4e1f-9e77-a7005137f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"NADI2023_MT_examples.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "970419b4-9a03-4025-a30f-7064e0252fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datasets.Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c9c29046-2418-4802-87cb-bb6920167641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.push_to_hub(\"NADI-Sample\", max_shard_size=\"500MB\", token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "358783c5-2427-482b-b69b-a7e3e7aec525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datasets.load_dataset(\"macabdul9/NADI-Sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3ab3f3f8-42a1-4dbf-a162-d440d3ef6e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oX-KNklNzxrr",
   "metadata": {
    "id": "oX-KNklNzxrr"
   },
   "source": [
    "- This is two prompt template / or wrapper we are going to use.\n",
    "- Some instruction contains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "12ce29f6-1938-42ae-a77e-7ad8e52c3306",
   "metadata": {
    "id": "12ce29f6-1938-42ae-a77e-7ad8e52c3306"
   },
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\":(\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"translation\":(\n",
    "        \"Input:{source_dialect}\\nOutput:{target_msa}\"\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd86f44-f95a-4e90-a488-60350eccf2fe",
   "metadata": {
    "id": "1cd86f44-f95a-4e90-a488-60350eccf2fe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f729a17-a9ca-4e53-9b4c-196834bfe5e1",
   "metadata": {
    "id": "8f729a17-a9ca-4e53-9b4c-196834bfe5e1"
   },
   "source": [
    "If we are adding any new tokens to then we need to extend the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ce8b518d-d9dc-4cd8-ac37-24641e0eedd0",
   "metadata": {
    "id": "ce8b518d-d9dc-4cd8-ac37-24641e0eedd0"
   },
   "outputs": [],
   "source": [
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-B7AIm2BvoZy",
   "metadata": {
    "id": "-B7AIm2BvoZy"
   },
   "source": [
    "### Tokenize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "75946c4e-28c9-4ed3-ad63-56bb9c4cc45c",
   "metadata": {
    "id": "75946c4e-28c9-4ed3-ad63-56bb9c4cc45c"
   },
   "outputs": [],
   "source": [
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2dcabd11-3dbe-41f1-9f90-f86fdd31ccd8",
   "metadata": {
    "id": "2dcabd11-3dbe-41f1-9f90-f86fdd31ccd8"
   },
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5ff8d308-0c54-4018-b6c4-adb767b53b50",
   "metadata": {
    "id": "5ff8d308-0c54-4018-b6c4-adb767b53b50"
   },
   "outputs": [],
   "source": [
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_name_or_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "\n",
    "        # Load the dataset\n",
    "        logging.warning(\"Loading data...\")\n",
    "        # 1. Load tsv file 2. Convert them into hf dataset\n",
    "        # df = pd.read_csv(dataset_name_or_path\n",
    "        dataset = datasets.load_dataset(dataset_name_or_path, split=split_name)\n",
    "\n",
    "\n",
    "        logging.warning(\"Formatting inputs...\")\n",
    "        # if there is no input for prompt the use prompt_no_input template else use prompt_input template\n",
    "        prompt_input, prompt_no_input, prompt_translation = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"], PROMPT_DICT[\"translation\"]\n",
    "        # sources = [\n",
    "        #     prompt_input.format_map(example) if example.get(input_col_name, \"\") != \"\" else prompt_no_input.format_map(example)\n",
    "        #     for example in tqdm(dataset)\n",
    "        # ]\n",
    "        sources = [prompt_translation.format_map(example) for example in dataset]\n",
    "        \n",
    "        targets = [f\"{example[output_col_name]}{tokenizer.eos_token}\" for example in dataset]\n",
    "\n",
    "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "        data_dict = preprocess(sources, targets, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "164bd107-ad15-4951-beb0-47442a072be7",
   "metadata": {
    "id": "164bd107-ad15-4951-beb0-47442a072be7"
   },
   "outputs": [],
   "source": [
    "# for example in dataset['train']:\n",
    "#     print(example.get(input_col_name, \"input\"))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f7e0e317-c18e-450e-bb54-666150c17c5e",
   "metadata": {
    "id": "f7e0e317-c18e-450e-bb54-666150c17c5e"
   },
   "outputs": [],
   "source": [
    "# dataset = SupervisedDataset(dataset_name_or_path=dataset_name_or_path, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8f5d2bc4-1d9e-46f0-a0ec-a50749fab4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_input, prompt_no_input, prompt_translation = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"], PROMPT_DICT[\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "24ba2191-9909-4c86-b2b7-c8d9e610b46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sources = [prompt_translation.format_map(example) for example in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1f1aaeb2-41b5-45f3-8357-115c915bd1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "L-lgnXWf0lRD",
   "metadata": {
    "id": "L-lgnXWf0lRD"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dLTcAZUU0ueb",
   "metadata": {
    "id": "dLTcAZUU0ueb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f8190af7-54eb-4d86-979b-0a5de8ae7f6e",
   "metadata": {
    "id": "f8190af7-54eb-4d86-979b-0a5de8ae7f6e"
   },
   "outputs": [],
   "source": [
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = SupervisedDataset(tokenizer=tokenizer, dataset_name_or_path=dataset_name_or_path)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "hvi5uWBa1emE",
   "metadata": {
    "id": "hvi5uWBa1emE"
   },
   "outputs": [],
   "source": [
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a57c9dde-4590-4dbe-9b48-0e6663cfe46c",
   "metadata": {
    "id": "a57c9dde-4590-4dbe-9b48-0e6663cfe46c"
   },
   "outputs": [],
   "source": [
    "def train(training_args):\n",
    "\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        cache_dir=cache_dir,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.config.use_cache=False\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        cache_dir=cache_dir,\n",
    "        model_max_length=model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        smart_tokenizer_and_embedding_resize(\n",
    "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "        )\n",
    "    if \"llama\" in model_name_or_path:\n",
    "        tokenizer.add_special_tokens(\n",
    "            {\n",
    "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    data_module = make_supervised_data_module(tokenizer=tokenizer)\n",
    "\n",
    "    # update training args to make output dir\n",
    "    output_dir = os.path.join(training_args.output_dir, model_name_or_path.split(\"/\")[-1])\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    training_args.output_dir = output_dir\n",
    "\n",
    "    trainer = Trainer(model=model, tokenizer=tokenizer, args=training_args, **data_module)\n",
    "\n",
    "    # resume from last checkpoint if it exists\n",
    "    checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "\n",
    "    if checkpoint:\n",
    "        print(f\"Checkpoint found! Training from {checkpoint} checkpoint!\")\n",
    "        trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    else:\n",
    "        print(f\"No checkpoint found! Training from scratch!\")\n",
    "        trainer.train()\n",
    "\n",
    "    # trainer.train()\n",
    "    # save states\n",
    "    trainer.save_state()\n",
    "    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n",
    "    print(f\"Training finished! Saved model to {training_args.output_dir}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28933e42-bd69-4939-84e3-8a40ee361b89",
   "metadata": {
    "id": "28933e42-bd69-4939-84e3-8a40ee361b89"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "aBHgmBOO1VWD",
   "metadata": {
    "id": "aBHgmBOO1VWD"
   },
   "outputs": [],
   "source": [
    "output_dir = \"output\" #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "PuMQtT6U1Qt1",
   "metadata": {
    "id": "PuMQtT6U1Qt1"
   },
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "RiJIyyfx51eq",
   "metadata": {
    "id": "RiJIyyfx51eq"
   },
   "outputs": [],
   "source": [
    "# transformers.TrainingArguments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7bd25061-8c21-4f20-9dc8-cfd20503e3fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "7bd25061-8c21-4f20-9dc8-cfd20503e3fa",
    "outputId": "fad32511-360f-4aa1-c62a-520691a84e0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found! Training from scratch!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 07:33, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.781700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.462300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.145100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.004300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished! Saved model to output/Jasmine-350M.\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "train(training_args=training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DePLcLnOWiGQ",
   "metadata": {
    "id": "DePLcLnOWiGQ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44feac4b-8b5b-412b-b5de-991adf5f18f6",
   "metadata": {
    "id": "44feac4b-8b5b-412b-b5de-991adf5f18f6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "327f9a00-cad9-4fc9-bd5d-22738573f34b",
   "metadata": {
    "id": "327f9a00-cad9-4fc9-bd5d-22738573f34b"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "OM_fAT-k7Oug",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OM_fAT-k7Oug",
    "outputId": "0459b897-4209-4c87-d684-e5006052dc52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITATION.bib  \u001b[0m\u001b[01;34mexamples\u001b[0m/   \u001b[01;34mlm_eval\u001b[0m/           pile_statistics.json  requirements.txt  \u001b[01;34mtemplates\u001b[0m/\n",
      "CODEOWNERS    ignore.txt  \u001b[01;34mlm_eval.egg-info\u001b[0m/  pyproject.toml        \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
      "\u001b[01;34mdocs\u001b[0m/         LICENSE.md  mypy.ini           README.md             setup.py\n"
     ]
    }
   ],
   "source": [
    "ls lm-evaluation-harness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3182e47-0897-41ae-821e-1b3b74c512a5",
   "metadata": {
    "id": "c3182e47-0897-41ae-821e-1b3b74c512a5"
   },
   "outputs": [],
   "source": [
    "# clone llm-evaluation-harness\n",
    "# !git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
    "# cd lm-evaluation-harness\n",
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XggnQZFSWpjl",
   "metadata": {
    "id": "XggnQZFSWpjl"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "SJIcaiXH2sME",
   "metadata": {
    "id": "SJIcaiXH2sME"
   },
   "source": [
    "#### Zero-shot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5BxFT2Q2q9R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5BxFT2Q2q9R",
    "outputId": "a195817e-3c7b-4e93-b39c-8eae56d9cf8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-24:05:24:58,724 INFO     [utils.py:160] NumExpr defaulting to 2 threads.\n",
      "2024-01-24:05:24:58,987 INFO     [config.py:58] PyTorch version 2.1.0+cu121 available.\n",
      "2024-01-24:05:24:58,988 INFO     [config.py:95] TensorFlow version 2.15.0 available.\n",
      "2024-01-24:05:24:58,989 INFO     [config.py:108] JAX version 0.4.23 available.\n",
      "2024-01-24 05:24:59.465533: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 05:24:59.465590: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 05:24:59.466821: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 05:25:00.661867: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-24:05:25:04,183 INFO     [__main__.py:156] Verbosity set to INFO\n",
      "2024-01-24:05:25:08,391 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.\n",
      "2024-01-24:05:25:12,437 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.\n",
      "2024-01-24:05:25:12,438 INFO     [__main__.py:229] Selected Tasks: ['sst2']\n",
      "2024-01-24:05:25:12,452 INFO     [huggingface.py:148] Using device 'cuda:0'\n",
      "2024-01-24:05:25:14,506 WARNING  [task.py:603] [Task: sst2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-01-24:05:25:14,506 WARNING  [task.py:615] [Task: sst2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-01-24:05:25:17,976 INFO     [task.py:340] Building contexts for task on rank 0...\n",
      "2024-01-24:05:25:18,436 INFO     [evaluator.py:319] Running loglikelihood requests\n",
      "  0% 0/1744 [00:00<?, ?it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
      "Determined largest batch size: 64\n",
      " 18% 321/1744 [00:04<00:10, 132.87it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
      "Determined largest batch size: 64\n",
      "100% 1744/1744 [00:10<00:00, 167.90it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=gpt2), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto:4 (64,64,64,64)\n",
      "|Tasks|Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
      "|-----|------:|------|-----:|------|-----:|---|-----:|\n",
      "|sst2 |      1|none  |     0|acc   |0.5505|±  |0.0169|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# zero-shot pre-trained model\n",
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=\"gpt2\" \\\n",
    "    --tasks sst2 \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size auto:4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de5d67e2-0bd8-4afa-8a38-69d39401383b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de5d67e2-0bd8-4afa-8a38-69d39401383b",
    "outputId": "93b1d16e-29df-474f-96d8-8b886e90c10d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-24:05:25:51,870 INFO     [utils.py:160] NumExpr defaulting to 2 threads.\n",
      "2024-01-24:05:25:52,137 INFO     [config.py:58] PyTorch version 2.1.0+cu121 available.\n",
      "2024-01-24:05:25:52,138 INFO     [config.py:95] TensorFlow version 2.15.0 available.\n",
      "2024-01-24:05:25:52,139 INFO     [config.py:108] JAX version 0.4.23 available.\n",
      "2024-01-24 05:25:52.615154: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 05:25:52.615202: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 05:25:52.616432: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 05:25:53.710163: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-01-24:05:25:56,950 INFO     [__main__.py:156] Verbosity set to INFO\n",
      "2024-01-24:05:26:02,001 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.\n",
      "2024-01-24:05:26:05,933 WARNING  [__init__.py:194] Some tasks could not be loaded due to missing dependencies. Run with `--verbosity DEBUG` for full details.\n",
      "2024-01-24:05:26:05,934 INFO     [__main__.py:229] Selected Tasks: ['sst2']\n",
      "2024-01-24:05:26:05,943 INFO     [huggingface.py:148] Using device 'cuda:0'\n",
      "2024-01-24:05:26:06,782 WARNING  [task.py:603] [Task: sst2] metric acc is defined, but aggregation is not. using default aggregation=mean\n",
      "2024-01-24:05:26:06,782 WARNING  [task.py:615] [Task: sst2] metric acc is defined, but higher_is_better is not. using default higher_is_better=True\n",
      "2024-01-24:05:26:10,511 INFO     [task.py:340] Building contexts for task on rank 0...\n",
      "2024-01-24:05:26:11,180 INFO     [evaluator.py:319] Running loglikelihood requests\n",
      "  0% 0/1744 [00:00<?, ?it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
      "Determined largest batch size: 64\n",
      " 18% 321/1744 [00:04<00:10, 130.42it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
      "Determined largest batch size: 64\n",
      "100% 1744/1744 [00:10<00:00, 162.45it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "hf (pretrained=/content/output/gpt2), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto:4 (64,64,64,64)\n",
      "|Tasks|Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
      "|-----|------:|------|-----:|------|-----:|---|-----:|\n",
      "|sst2 |      1|none  |     0|acc   |0.5998|±  |0.0166|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=/content/output/gpt2 \\\n",
    "    --tasks sst2 \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size auto:4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eadf1f-7b96-4799-a550-d166ff487d76",
   "metadata": {
    "id": "05eadf1f-7b96-4799-a550-d166ff487d76"
   },
   "source": [
    "### Prompting\n",
    "- Once we have trained the model to follow instructions, we can prompt that model to generate a response.\n",
    "- We will be using HF's generation pipeline to prompt our trained model.\n",
    "- After training the model with a particular prompt wrapper it is advised to use the same prompt format during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bef69ec5-8f1d-4bcb-a988-987a853ec9e1",
   "metadata": {
    "id": "bef69ec5-8f1d-4bcb-a988-987a853ec9e1"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64253dce-fe52-42c5-abd2-3f84a7f70b0f",
   "metadata": {
    "id": "64253dce-fe52-42c5-abd2-3f84a7f70b0f"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer and trained model and then create chatbot pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/output/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/content/output/gpt2\", device_map=\"cuda:0\")\n",
    "\n",
    "chatbot = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ZJiy2iFW_0Tz",
   "metadata": {
    "id": "ZJiy2iFW_0Tz"
   },
   "outputs": [],
   "source": [
    "# pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "EhL-B70DXjS1",
   "metadata": {
    "id": "EhL-B70DXjS1"
   },
   "outputs": [],
   "source": [
    "# format the prompt\n",
    "text = \"What is Machine Learning?\"\n",
    "\n",
    "prompt = PROMPT_DICT['prompt_no_input'].format(instruction=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "iUYctoAK79Wq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "iUYctoAK79Wq",
    "outputId": "9a6ef518-d7f6-42d8-ff65-04b1e6ec5209"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is Machine Learning?\\n\\n### Response:'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t3Ma2Y_-YR5A",
   "metadata": {
    "id": "t3Ma2Y_-YR5A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea89e1a2-7c57-4006-8cf4-426199a0318a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea89e1a2-7c57-4006-8cf4-426199a0318a",
    "outputId": "10cb5390-b5e2-415b-d82e-cfef9bd9ab6f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "sequences = chatbot(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.4,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=64,\n",
    "    return_full_text=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qg79HroWZe7Y",
   "metadata": {
    "id": "Qg79HroWZe7Y"
   },
   "outputs": [],
   "source": [
    "# !ps ux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ecf3e3f-4553-44df-b00c-318fec4ae96a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ecf3e3f-4553-44df-b00c-318fec4ae96a",
    "outputId": "81c18e1f-3e52-4044-9297-e18d90a5c713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a new field of research that has been gaining momentum in recent years. Machine learning is a new field of research that has been gaining momentum in recent years. Machine learning is a new field of research that has been gaining momentum in recent years.\n",
      "\n",
      "Machine learning is a new field of research that has been\n"
     ]
    }
   ],
   "source": [
    "print(sequences[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c04e1-d302-4f2e-8bce-ae0f26f03d26",
   "metadata": {
    "id": "733c04e1-d302-4f2e-8bce-ae0f26f03d26"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53f136b1-574a-46ff-8ade-5565db02229e",
   "metadata": {
    "id": "53f136b1-574a-46ff-8ade-5565db02229e"
   },
   "source": [
    "#### Next Tutorial - Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0a0fe-4416-481b-8d03-b9cb89889204",
   "metadata": {
    "id": "01e0a0fe-4416-481b-8d03-b9cb89889204"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
