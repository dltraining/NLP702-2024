{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 6: LoRA (Low Rank Adaptation)\n",
    "\n",
    "Welcome to this week's lab. We will be learning about [LoRA](https://arxiv.org/abs/2106.09685) which has been quite popular lately!\n",
    "\n",
    "Objectives in this lab are as follow:\n",
    "\n",
    "1. Better understanding of LoRA through code implementation.\n",
    "2. Minimal implementation of LoRA for training.\n",
    "\n",
    "Have fun! 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from typing import Optional, List\n",
    "\n",
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(tokenizer, model, system_prompt, user_prompt, device='cuda', max_new_tokens=256):\n",
    "    # Format our input to model's instruction-following format\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer([text], return_tensors=\"pt\").to(device).input_ids\n",
    "    # Pass the input to model\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "    # Parse output\n",
    "    output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return output\n",
    "\n",
    "# For training dataset\n",
    "def convert_to_qwen_format(data, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": data['user']},\n",
    "        {\"role\": \"assistant\", \"content\": data['assistant']}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def tokenize(x, tokenizer):\n",
    "    return tokenizer(x['text'])\n",
    "\n",
    "def prepare_dataset(datapath, tokenizer):\n",
    "    df = pd.read_csv(datapath)\n",
    "    df['text'] = df.apply(lambda x: convert_to_qwen_format(x, tokenizer), axis=1)\n",
    "    train_data = datasets.Dataset.from_pandas(df)\n",
    "    train_dataset = train_data.map(lambda x: tokenize(x, tokenizer), batched=True, batch_size=16, remove_columns=df.columns.to_list())\n",
    "    return train_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.adapterhub.ml/_images/lora.png\" alt=\"lora-diagram\" width=\"200\"/>\n",
    "\n",
    "Denote each hidden layer in our model as $h = Wx$, where $x$ and $W$ is the input and the weight of the layer.\n",
    "\n",
    "LoRA modules reparametrize $h$, so that $h = Wx + \\frac{\\alpha}{r}BAx$.\n",
    "\n",
    "$A \\in \\mathbb{R}^{d \\times r}$, $B \\in \\mathbb{R}^{r \\times k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we have the following model\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W0 = nn.Linear(256, 128)\n",
    "        self.W1 = nn.Linear(128, 64)\n",
    "        self.W2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "    ):\n",
    "        h0 = self.W0(x)\n",
    "        h1 = self.W1(h0)\n",
    "        h2 = self.W2(h1)\n",
    "        return h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Say we want to inject LoRA modules in the hidden layers\n",
    "# in layer \"W1\". How we were about to do that?\n",
    "class MyModelWithLoRA(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size, r, out_size, a\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W0 = nn.Linear(256, 128)\n",
    "        self.W1 = nn.Linear(128, 64)\n",
    "        self.W2 = nn.Linear(64, 1)\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.a = a\n",
    "        self.r = r\n",
    "        self.k = out_size\n",
    "        self.A = nn.Linear(input_size, r)\n",
    "        self.B = nn.Linear(r, out_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "    ):\n",
    "        h0 = self.W0(x)\n",
    "        h1 = ...\n",
    "        h2 = self.W2(h1)\n",
    "        return h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1]) torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "# Forward-pass test\n",
    "my_model = MyModel()\n",
    "my_model_w_lora = MyModelWithLoRA(input_size=..., r=..., out_size=..., a=...)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.randn([2, 256])\n",
    "    \n",
    "    print(my_model(x).shape, my_model_w_lora(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Let's prepare our base model first.\n",
    "# We will use an instruction-following LLM, Qwen, as our model.\n",
    "device = \"cuda\" \n",
    "model_name = \"Qwen/Qwen1.5-0.5B-Chat\" \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful AI companion.\n",
      "user\n",
      "What is the difference between anxiousness and discomfort? Answer it concisely.\n",
      "assistant\n",
      "Anxiety and discomfort can be defined as two different emotions that are often associated with feeling nervous or uneasy about something. Anxiety is characterized by feelings of unease, fear, or worry about the future, while discomfort is characterized by physical symptoms such as pain, pain in the joints, and difficulty moving. While anxiety can sometimes be related to certain situations or events, discomfort is more likely to arise when an individual is facing something that they feel uncomfortable with or uncertain about.\n"
     ]
    }
   ],
   "source": [
    "# As our model is instruction-following\n",
    "# we can perform prompt / instruction-based text generation as follows:\n",
    "system_prompt = \"You are a helpful AI companion.\"\n",
    "user_prompt = \"What is the difference between anxiousness and discomfort? Answer it concisely.\"\n",
    "print(generate(system_prompt, user_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's examine the model's architecture\n",
    "# our base model is a transformer-based decoder-only LLM \n",
    "# in this type of model, it is common to inject the LoRA (or other adapter)\n",
    "# into its \"embedding\" layers or linear layers within the transformer blocks.\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA for Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: LoRA modules reparametrize $h$, so that $h = Wx + \\frac{\\alpha}{r}BAx$.\n",
    "\n",
    "$A \\in \\mathbb{R}^{d \\times r}$, $B \\in \\mathbb{R}^{r \\times k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement a LoRA module for embedding layer\n",
    "class EmbeddingLoRA(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size, r, out_size, a,\n",
    "        W,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Original weights\n",
    "        self.W = W\n",
    "\n",
    "        # Defining LoRA layers\n",
    "        self.d = input_size\n",
    "        self.a = a\n",
    "        self.r = r\n",
    "        self.k = out_size\n",
    "        self.A = ...\n",
    "        self.B = ...\n",
    "\n",
    "        self.use_lora = True\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "    ):\n",
    "        # Forward-pass\n",
    "        h =  self.W(x)\n",
    "        if self.use_lora:\n",
    "            ...\n",
    "        return h\n",
    "\n",
    "    def save_lora_weights(\n",
    "        self,  \n",
    "        savepath,\n",
    "    ):\n",
    "        torch.save({\"A\": A.state_dict(), \"B\": B.state_dict()}, savepath)\n",
    "\n",
    "    def load_weights(\n",
    "        self,\n",
    "        loadpath,\n",
    "    ):\n",
    "        weights = torch.load(loadpath, map_location=\"cpu\").to(self.device)\n",
    "        self.A.load_state_dict(weights[\"A\"])\n",
    "        self.B.load_state_dict(weights[\"B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LoRA embedding layer\n",
    "lora_embedding = EmbeddingLoRA(\n",
    "    input_size=...,\n",
    "    k=...,\n",
    "    out_size=...,\n",
    "    a=...,\n",
    "    W=...,\n",
    ")\n",
    "_ = lora_embedding.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inject LoRA embedding to base model\n",
    "model.model.embed_tokens = lora_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): EmbeddingLoRA(\n",
       "      (W): Embedding(151936, 1024)\n",
       "      (A): Embedding(151936, 128)\n",
       "      (B): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers \n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "# Freeze all layers but LoRA\n",
    "for name, params in model.model.embed_tokens.named_parameters():\n",
    "    if \"W\" not in name:\n",
    "        params.requires_grad = True\n",
    "\n",
    "total_params = sum(param.numel() for param in model.parameters())\n",
    "trainable_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "print(f\"Total Params    : {total_params:,}\")\n",
    "print(f\"Trainable Params: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to train the model\n",
    "\n",
    "# Fetch the dataset\n",
    "train_dataset = prepare_dataset(\"mbzuai.csv\")\n",
    "\n",
    "# Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test-mbz-data\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_total_limit=1,\n",
    "    learning_rate=1e-4\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful AI companion.\n",
      "user\n",
      "What is mbzuai?\n",
      "assistant\n",
      "mbzuai is a popular web scraping tool that allows users to extract data from websites using their web browsers. It can be used for various purposes, such as data collection, website analysis, and user profiling.\n"
     ]
    }
   ],
   "source": [
    "# Generate some text\n",
    "system_prompt = \"You are a helpful AI companion.\"\n",
    "user_prompt = \"What is mbzuai?\"\n",
    "print(generate(system_prompt, user_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the result is not quite good. Let's explore injecting LoRA into Linear layers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA for Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement a LoRA module for linear layer\n",
    "class LinearLoRA(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size, r, out_size, a,\n",
    "        W,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Original weights\n",
    "        self.W = W\n",
    "\n",
    "        # Defining LoRA layers\n",
    "        self.input_size = input_size\n",
    "        self.a = a\n",
    "        self.r = r\n",
    "        self.out_size = out_size\n",
    "        self.A = ...\n",
    "        self.B = ...\n",
    "\n",
    "        self.use_lora = True\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "    ):\n",
    "        # Forward-pass\n",
    "        h =  self.W(x)\n",
    "        if self.use_lora:\n",
    "            ...\n",
    "        return h\n",
    "\n",
    "    def save_lora_weights(\n",
    "        self,  \n",
    "        savepath,\n",
    "    ):\n",
    "        torch.save({\"A\": A.state_dict(), \"B\": B.state_dict()}, savepath)\n",
    "\n",
    "    def load_weights(\n",
    "        self,\n",
    "        loadpath,\n",
    "    ):\n",
    "        weights = torch.load(loadpath, map_location=\"cpu\").to(self.device)\n",
    "        self.A.load_state_dict(weights[\"A\"])\n",
    "        self.B.load_state_dict(weights[\"B\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Injecting linear LoRA recusively to each layers\n",
    "for idx in range(len(model.model.layers)):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): EmbeddingLoRA(\n",
       "      (W): Embedding(151936, 1024)\n",
       "      (A): Embedding(151936, 128)\n",
       "      (B): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): LinearLoRA(\n",
       "            (W): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (A): Linear(in_features=1024, out_features=128, bias=True)\n",
       "            (B): Linear(in_features=128, out_features=1024, bias=True)\n",
       "          )\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, our model has been injected with more LoRAs\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params    : 489,886,720\n",
      "Trainable Params: 25,899,008\n"
     ]
    }
   ],
   "source": [
    "# Freeze all layers \n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "# Freeze all layers but LoRA\n",
    "for name, params in model.model.embed_tokens.named_parameters():\n",
    "    if \"W\" not in name:\n",
    "        params.requires_grad = True\n",
    "for idx in range(len(model.model.layers)):\n",
    "    ...\n",
    "\n",
    "total_params = sum(param.numel() for param in model.parameters())\n",
    "trainable_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "print(f\"Total Params    : {total_params:,}\")\n",
    "print(f\"Trainable Params: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ed70cb4a334eebb16eaf8215841441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:18, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=4.149049682617187, metrics={'train_runtime': 19.2214, 'train_samples_per_second': 65.032, 'train_steps_per_second': 5.203, 'total_flos': 149886152644608.0, 'train_loss': 4.149049682617187, 'epoch': 50.0})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try to train the model\n",
    "\n",
    "# Fetch the dataset\n",
    "train_dataset = prepare_dataset(\"mbzuai.csv\")\n",
    "\n",
    "# Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test-mbz-data\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_total_limit=1,\n",
    "    learning_rate=1e-4\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are a helpful AI companion.\n",
      "user\n",
      "What is MBZUAI?\n",
      "assistant\n",
      "MBZUAI stands for Membangun Anak Kewangan Uji (Malaysian的意思是“儿童教育中心”。它是一个非营利组织，旨在为当地贫困家庭的孩子提供免费的教育机会。它的目标是在贫困地区建立一个学习环境，并确保每个孩子都有接受良好教育的机会。\n"
     ]
    }
   ],
   "source": [
    "# Generate some text\n",
    "system_prompt = \"You are a helpful AI companion.\"\n",
    "user_prompt = \"What is MBZUAI?\"\n",
    "print(generate(system_prompt, user_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging LoRA Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can merge the LoRA weights with the original pretrained weights.\n",
    "\n",
    "As,  $h = Wx + \\frac{\\alpha}{r}BAx = (W + \\frac{\\alpha}{r}BA) x$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to create the function to merge weights\n",
    "# and integrate it to the code\n",
    "\n",
    "def merge_weights(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a0cd12b50a50342ace14d034ed660b8ec1ba6849d6f6ee524b6e2fdf5abc3965"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
